\documentclass[a4paper,10pt]{article}

\usepackage{geometry}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{cleveref} % Pour pouvoir utiliser cref

\theoremstyle{definition} % Style pour les définitions
\newtheorem{definition}{Définition}[section]

\theoremstyle{definition} % Style pour les propositions et lemmes
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{lemma}[definition]{Lemme}

\theoremstyle{definition} % Style pour les remarques
\newtheorem{theorem}[definition]{Théorème}

\theoremstyle{definition} % Style pour les remarques
\newtheorem{remark}[definition]{Remarque}

\newcommand{\E}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Tc}{\mathcal{T}}
\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Ac}{\mathcal{A}}
\newcommand{\dist}{\mathrm{dist}}
\newcommand{\eqdef}{\stackrel{\mathrm{def}}{=}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\xmap}{x_{\scalebox{0.5}{\textrm{MAP}}}}
\newcommand{\xmmse}{x_{\scalebox{0.5}{\textrm{MMSE}}}}
\newcommand{\xmarginal}{x_{\scalebox{0.5}{\textrm{MARG}}}}
\newcommand{\todo}[1]{\textcolor{red}{\textbf{TODO:} #1}}

\title{MSE equivariant et local}
\author{DO Quoc Bao}
\date{April 2025}

\begin{document}

\maketitle

Le débruitage des images est une tâche importante dans plusieurs domaines. Dans ce papier, on cherche le comportement d'un réseau de débruitage sous les contraintes de localité et d'équivariance par translation. 

\section{Définition du problème}
Considérons un signal \( X \in \mathbb{R}^N \) et une mesure bruitée \( Y \in \mathbb{R}^N \) :  
\[
Y = A X + B,
\]  
où \( A \in \R^{M\times N} \) est une matrice à valeurs dans $\R$, et \( B \in \mathbb{R}^M \) représente un bruit blanc gaussien, i.e \( B \sim \mathcal{N}(0, \sigma^{2} I_M) \), où \( \sigma \geq 0 \).

L'objectif est de concevoir une fonction de reconstruction qui à $Y$ associe un estimateur $\hat X$ de $X$. 
Pour ce faire, on considère deux structures différentes. 

\paragraph{Cas 1 : inversion complète}
Dans ce cas, on considère une fonction \( \phi : \mathbb{R}^M \to \mathbb{R}^N \) qui minimise l'erreur quadratique moyenne attendue:  
\[
\inf_{\phi\,\in\, \Phi} \, \mathbb{E} \left[ \| \phi(Y) - X \|^2 \right],
\] 
avec $\Phi$ un sous-espace vectoriel de fonctions. 

\paragraph{Cas 2 : débruiteur spécifique}
Une autre possibilité populaire est de considérer une structure particulière de la forme :
\[
\inf_{\psi\,\in\, \Psi} \, \mathbb{E} \left[ \| \psi(A^+Y) - X \|^2 \right],
\] 
avec $\Psi$ un sous-espace vectoriel de fonctions et où $A^+$ est la pseudo-inverse de $A$. 
Une différence importante ici est que $\psi$ est entraîné sur $\mathrm{Im}(A^T)$ uniquement.

\paragraph{Classes de fonctions $\Phi$ et $\Psi$}

Nous nous intéresserons particulièrement la classe $\Phi$ des fonctions équivariantes par des éléments d'un groupe et qui agissent localement.
L'équivariance peut être exprimée ainsi :
\begin{equation}
    \phi(T_\tau Y) = S_\tau \phi(Y)
\end{equation}
pour des couples de transformations $(T_\tau, S_\tau)_{\tau}$.

Dans le second cas, l'équivariance s'exprime sous la forme :
\begin{equation}
    \psi(T_\tau X) = T_\tau \psi(X) \quad  \forall X\in \R^N
\end{equation}

\todo{Compléter en définissant la notion de localité}

\todo{Compléter avec cette notion d'intertwinner. }

\todo{Bien séparer l'analyse avec $\phi(A^+)$ et $\phi$}


\section{Conditions d'optimalité dans le cas 2}

On définit le risque moyen par :
\begin{equation}
    J(\psi) = \mathbb{E} \left[ \| \psi(A^{+}Y) - X \|^2 \right].
\end{equation}

Le risque empirique est obtenu lorsque $p_X= \frac{1}{I}\sum_{i=1}^I \delta_{x_i}$. Dans ce cas : 
\begin{equation}
    J(\psi) = \frac{1}{I}\sum_{i=1}^I  \mathbb{E} \| \phi(A^{+}Ax_i + A^+B) - x_i  \|^2.
\end{equation}

On cherche $\psi^\star$ le minimiseur du risque moyen ou du risque empirique sur l'ensemble $\Psi$.
\begin{equation*}
    \psi^\star = \argmin_{\psi\,\in\, \Psi} J(\psi).
\end{equation*}

\subsection{Minimisation sous contrainte de localité}

Le sous-espace vectoriel $\Psi$ est l'ensemble de fonctions agissant localement c.-à-d. :
\begin{equation*}
    \Psi = \left\{\psi : \mathbb{R}^N \to \mathbb{R}^d \text{ telles que } \exists (v_k : \mathbb{R}^d \to \mathbb{R})_{k \in [1, N]}, \forall z \in \mathbb{R}^N, \psi(z)[k] = v_k(z_{\Omega_k})\right\}
\end{equation*}


Comme $\Psi$ est un sous-espace vectoriel, les conditions d'optimalité du premier ordre s'écrivent :
\begin{equation}
    DJ(\psi^\star)(h) = 0 \quad \forall h \in \Psi,
\end{equation}
où $DJ$ est la dérivée au sens de Gâteaux de $J$.


Ainsi,
\begin{equation*}
    \frac{2}{T} \sum_{i=1}^I \int_{\mathbb{R}^M} \langle \psi^*(A^+A x_i + A^+b) - x_i, h(A^+A x_i + A^+b) \rangle G_{\sigma^2}(b) \, db = 0.
\end{equation*}


On fixe $k \in [1, N]$, prenons $h(z) = (h_1(z), h_2(z), \dots, h_N(z))$ définie par
\begin{equation}
    h_j(z) =
\begin{cases}
0 & \text{si } j \neq k \\
\delta(z_{\Omega_k} - \theta) & \text{si } j = k
\end{cases}
\end{equation}


Par construction, $h$ admet la propriété de localité, $h$ est donc dans $\Psi$.

On en obtient :
\begin{equation*}
\sum_{i=1}^I \int_{\mathbb{R}^M} (\psi^*(A^+A x_i + A^+b)[k] - x_i[k]) \,\delta(A^+A x_i + A^+b)_{\Omega_k} - \theta) G_{\sigma^2}(b) \, db = 0.    
\end{equation*}


Comme $\psi^* \in \Psi$, on peut modéliser $\psi^*(A^+A x_i + A^+b)[k] = \psi_k^*\left((A^+A x_i + A^+b)_{\Omega_k}\right)$.

D'où :
\begin{equation*}
\sum_{i=1}^I \int_{\mathbb{R}^M} \left(\psi^*_k(A^+A x_i + A^+b)_{\Omega_k}\right) \delta\left((A^+A x_i + A^+b)_{\Omega_k} - \theta\right) G_{\sigma^2}(b) \, db = 0.    
\end{equation*}


En rangeant :
\begin{equation*}
\psi^*_k(\theta)\sum_{i=1}^I \int_{\mathbb{R}^M} \,\delta\left((A^+A x_i + A^+b)_{\Omega_k} - \theta\right) G_{\sigma^2}(b) \, db = \sum_{i=1}^I x_i[k] \int_{\mathbb{R}^M} \delta\left((A^+A x_i + A^+b)_{\Omega_k} - \theta\right) G_{\sigma^2}(b) \, db    
\end{equation*}


D'où :
\begin{equation*}
\psi^*_k(\theta) = \frac{\sum_{i=1}^I x_i[k] \int_{\mathbb{R}^M} \delta\left((A^+A x_i + A^+b)_{\Omega_k} - \theta\right) G_{\sigma^2}(b) \, db}{\sum_{j=1}^I \int_{\mathbb{R}^M} \delta\left((A^+A x_j + A^+b)_{\Omega_k} - \theta\right) G_{\sigma^2}(b) \, db}.    
\end{equation*}


Considérons $q(x_i, \theta, k) = \int_{\mathbb{R}^M} \delta\left((A^+A x_i + A^+b)_{\Omega_k} - \theta\right) G_{\sigma^2}(b) \, db$.

On cherche $b \in \mathbb{R}^M$ telle que $(A^+A x_i + A^+b)_{\Omega_k} = \theta$, i.e
\begin{equation*}
(A^+)_{\Omega_k} A x_i + A^+ b_{\Omega_k} = \theta    
\end{equation*}


On pose $C$ la matrice dont son pseudo-inverse est $C^+ \coloneqq (A^+)_{\Omega_k}$.

D'ou,
\begin{equation*}
    b \in C (\theta - C^+ A x_i) + \ker(C^T)
\end{equation*}

% Le problème se pose sur l'unicité de l'ensemble admissible de $b$, car $C$ n'est pas inversible.

Ainsi :
\begin{align*}
    q(x_i, \theta, k) &= \int_{\ker(C^T) + C(\theta - C^+ A x_i)} G_{\sigma^2}(b) \, db \\
    &= \int_{\ker(C^T)} G_{\sigma^2}(b + C(\theta - C^+ A x_i)) \, db.
\end{align*}


Posons $ \mu_i^\theta \coloneqq c(\theta - C^+ A x_i)$.

Alors :
\[
q(x_i, \theta, k) = \int_{\ker(C^T)} G_{\sigma^2}(b + u_i^\theta) \, db.
\]

\begin{proposition}\label{prop:integral_gaussien_sousespace}
Soit \( X \sim \mathcal{N}(\mu, \Sigma) \) un vecteur gaussien de dimension \( n \), et soit \( S \subseteq \mathbb{R}^n \) un sous-espace vectoriel de dimension \( k \leq n \). Alors :
\[
\int_S \mathcal{N}(x; \mu, \Sigma) \, \mathrm{d}x = (2\pi)^{-\frac{n - k}{2}} |\Sigma|^{-1/2} |A|^{-1/2} \exp\left(\frac{1}{2} b^TA^{-1}b - \frac{1}{2}\mu^T \Sigma^{-1}\mu\right) ,
\]
où :
\begin{itemize}
    \item \( B \in \mathbb{R}^{n \times k} \) est une base orthonormale de \( S \),
    \item \( A = B^\top \Sigma^{-1} B \)
    \item  $b = B^T \Sigma^-1 \mu$
\end{itemize}
\end{proposition}

\begin{proof}

Soit \( B \) une base orthonormale de \( S \). Pour tout \( x \in S \), il existe un élément$z \in \R^k$ \( x = Bz \).

La densité devient :
\begin{equation*}
\mathcal{N}(Bz; \mu, \Sigma) = (2\pi)^{-n/2} |\Sigma|^{-1/2} \exp\left(-\frac{1}{2} (Bz - \mu)^\top \Sigma^{-1} (Bz - \mu)\right).    
\end{equation*}


Notons \( A = B^\top \Sigma^{-1} B \) et \( b = B^\top \Sigma^{-1} \mu \). L'exposant se réécrit :
\begin{equation*}
-\frac{1}{2} (z - A^{-1}b)^\top A (z - A^{-1}b) + \frac{1}{2} b^\top A^{-1} b - \frac{1}{2} \mu^\top \Sigma^{-1} \mu.    
\end{equation*}


L'intégrale sur \( z \) de la fonction de densite s'ecrit:
\begin{align*}
 L &=\int_{\R^k} (2\pi)^{-n/2} |\Sigma|^{-1/2} \exp\left(-\frac{1}{2} (z - A^{-1}b)^\top A (z - A^{-1}b) + \frac{1}{2} b^\top A^{-1} b - \frac{1}{2} \mu^\top \Sigma^{-1} \mu\right) |B^T B| \,dz\\
 &= (2\pi)^{-n/2} |\Sigma|^{-1/2} \exp\left(\frac{1}{2} b^\top A^{-1} b - \frac{1}{2} \mu^\top \Sigma^{-1} \mu\right) \int_{\R^k}  \exp\left(-\frac{1}{2} (z - A^{-1}b)^\top A (z - A^{-1}b) \right) dz \\
 &= (2\pi)^{-n/2} |\Sigma|^{-1/2} \exp\left(\frac{1}{2} b^\top A^{-1} b - \frac{1}{2} \mu^\top \Sigma^{-1} \mu\right) \left( (2\pi)^{k/2} |A|^{-1/2}\right)\\
 &= (2\pi)^{-\frac{n - k}{2}} |\Sigma|^{-1/2} |A|^{-1/2} \exp\left(\frac{1}{2} b^TA^{-1}b - \frac{1}{2}\mu^T \Sigma^{-1}\mu\right)
\end{align*}

\end{proof}

Supposons que $\ker(C^T)$ est engendré par une base orthogonale $S \in \mathbb{R}^{M \times r}$.

% Alors, pour tout $b \in \ker(C^T)$, il existe $z \in \mathbb{R}^r$ tel que $b = Sz$.

Ainsi, par \cref{prop:integral_gaussien_sousespace}, on obtient
\begin{equation*}
    q(x_i, \theta, k) = \frac{1}{(2\pi)^\frac{(M-r)}{2} \sigma^M} \exp\left(\frac{1}{2\sigma^2} (\mu_i^\theta)^T (SS^T - I) \mu_i^\theta\right).
\end{equation*}

\subsection{Minimisation sous la contrainte de localité et d'équivariance par translation}
Le sous-espace vectoriel $\Psi$ est l'ensemble de fonctions agissant localement, c.-à-d. :
\begin{equation*}
    \Psi = \left\{\psi : \mathbb{R}^N \to \mathbb{R}^d \text{ telles que } \exists\, v : \mathbb{R}^d \to \mathbb{R}, \forall z \in \mathbb{R}^N, \psi(z)[k] = v(z_{\Omega_k})\right\}
\end{equation*}
Lorsqu'on ajoute la contrainte d'équivariance par translation, on remarque la seule différence de $\Psi$ est que chaque composante de $\psi$ utilise une même fonction.

Dans le cas où la seule contrainte de localité est imposée, on a,
\begin{equation*}
    \psi(z)[k] = v_k(z_{\Omega_k})
\end{equation*}

Notons $\mathcal{I} \coloneqq {1, \dots , N}$, la suite de fonctions $(v_k)_{k\in \mathcal{I}}$ doit satisfaire quelques conditions supplémentaires lorsqu'on ajoute la contrainte d'équivariance. Soit $\tau$ une translation quelconque, une fonction $\psi$ équivariante vérifie
\begin{equation*}
    \psi(\tau z) = \tau\,\psi(z)
\end{equation*}

On en déduit
\begin{align*}
    \psi(\tau z)[k] &= (\tau \, \psi(z))[k], \quad \forall k\in\mathcal{I}\\
    v_k ((\tau z)_{\Omega_k}) &= v_{(\tau \mathcal{I})[k]}\left(z_{\Omega_{(\tau \mathcal{I})[k]}}\right), \quad \forall k\in\mathcal{I}\\
    v_k ( z_{\Omega_(\tau \mathcal{I})[k]}) &= v_{(\tau \mathcal{I})[k]}\left(z_{\Omega_{(\tau \mathcal{I})[k]}}\right), \quad \forall k\in\mathcal{I}
\end{align*}

Cette condition s'impose pour toute translation $\tau$ et pour tout $z \in \R^N$, ce qui implique qu'il existe une fonction commune $v$ telle que $v_k \equiv f, \,\forall k\in \mathcal{I}$.

Ensuite, comme $\Psi$ est un sous-espace vectoriel, les conditions d'optimalité du premier ordre s'écrivent :
\begin{equation}
    DJ(\psi^\star)(h) = 0 \quad \forall h \in \Psi,
\end{equation}
où $DJ$ est la dérivée au sens de Gâteaux de $J$.


Ainsi,
\begin{equation*}
    \frac{2}{T} \sum_{i=1}^I \int_{\mathbb{R}^M} \langle \psi^*(A^+A x_i + A^+b) - x_i, h(A^+A x_i + A^+b) \rangle G_{\sigma^2}(b) \, db = 0.
\end{equation*}


Prenons la fonction $h$ de sorte que
\begin{equation}
    h(z)[k] = \delta(z_{\Omega_k}-\theta)
\end{equation}


Par construction, $h$ est dans $\Psi.$

On en obtient :
\begin{equation*}
\sum_{i=1}^I \sum_{k=1}^N \int_{\mathbb{R}^M} (\psi^*(A^+A x_i + A^+b)[k] - x_i[k]) \,\delta(A^+A x_i + A^+b)_{\Omega_k} - \theta) G_{\sigma^2}(b) \, db = 0.    
\end{equation*}


Comme $\psi^* \in \Psi$, on peut modéliser $\psi^*(A^+A x_i + A^+b)[k] = f\left((A^+A x_i + A^+b)_{\Omega_k}\right)$.

D'où :
\begin{equation*}
\sum_{i=1}^I \sum_{k=1}^N \int_{\mathbb{R}^M} \left(f(A^+A x_i + A^+b)_{\Omega_k}\right) \delta\left((A^+A x_i + A^+b)_{\Omega_k} - \theta\right) G_{\sigma^2}(b) \, db = 0.    
\end{equation*}


En rangeant :
\begin{equation*}
f(\theta)\sum_{i=1}^I \sum_{k=1}^N \int_{\mathbb{R}^M} \,\delta\left((A^+A x_i + A^+b)_{\Omega_k} - \theta\right) G_{\sigma^2}(b) \, db = \sum_{i=1}^I \sum_{k=1}^N x_i[k] \int_{\mathbb{R}^M} \delta\left((A^+A x_i + A^+b)_{\Omega_k} - \theta\right) G_{\sigma^2}(b) \, db    
\end{equation*}


D'où :
\begin{equation*}
f(\theta) = \frac{\sum_{i=1}^I \sum_{k=1}^N x_i[k] \int_{\mathbb{R}^M} \delta\left((A^+A x_i + A^+b)_{\Omega_k} - \theta\right) G_{\sigma^2}(b) \, db}{\sum_{j=1}^I \sum_{k=1}^N \int_{\mathbb{R}^M} \delta\left((A^+A x_j + A^+b)_{\Omega_k} - \theta\right) G_{\sigma^2}(b) \, db}.    
\end{equation*}

On obtient le résultat final,
\begin{equation*}
\psi^\star(y)[l]=f(y_{\Omega_l}) = \sum_{i=1}^I \sum_{k=1}^N x_i[k]\frac{ q(x_i,y_{\Omega_l},k)}{\sum_{j=1}^I \sum_{k=1}^N q(x_j,y_{\Omega_l},k)}.    
\end{equation*}
\end{document}
