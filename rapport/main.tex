\documentclass[a4paper,10pt]{article}

\usepackage{geometry}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{cleveref} % Pour pouvoir utiliser cref

\theoremstyle{definition} % Style pour les définitions
\newtheorem{definition}{Définition}[section]

\theoremstyle{definition} % Style pour les propositions et lemmes
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{lemma}[definition]{Lemma}

\theoremstyle{definition} % Style pour les remarques
\newtheorem{theorem}[definition]{Theorem}

\theoremstyle{definition} % Style pour les remarques
\newtheorem{remark}[definition]{Remark}

\newcommand{\E}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Ac}{\mathcal{A}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\dist}{\mathrm{dist}}
\newcommand{\eqdef}{\stackrel{\mathrm{def}}{=}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\xmap}{x_{\scalebox{0.5}{\textrm{MAP}}}}
\newcommand{\xmmse}{x_{\scalebox{0.5}{\textrm{MMSE}}}}
\newcommand{\xmarginal}{x_{\scalebox{0.5}{\textrm{MARG}}}}
% Linear algebra
\newcommand{\Normal}[1]{\mathcal{N}\left( {#1} \right)}
\renewcommand{\ker}[1]{\mathrm{ker}\left( {#1} \right)}
\newcommand{\supp}[1]{\mathrm{supp}\left( {#1} \right)}
\newcommand{\Id}{\mathrm{I}}
\newcommand{\trace}[1]{\mathrm{tr}\left( #1 \right)}
\newcommand{\diag}[1]{\mathrm{diag}\left( #1 \right)}
\newcommand{\diam}[1]{\mathrm{diam}\left( #1 \right)}
\newcommand{\norm}[1]{\left\| #1 \right \|}
\newcommand{\inner}[1]{\left\langle #1 \right\rangle}


\newcommand{\todo}[1]{\textcolor{red}{\textbf{TODO:} #1}}


\title{An Analysis of Locality and Equivariant on Diffusion Model and MMSE Denoiser}
\author{Quoc-Bao DO}
\date{Fevrier 2024}

% \setlength{\parskip}{1em} % Pour choisir l'espace entre paragraphes

\begin{document}

\maketitle


\tableofcontents

\newpage
\section*{Abstract}
Convolutional Neural Networks (CNNs) have achieved remarkable success in image processing tasks such as face generation, inpainting and super-resolution. Despite their empirical effectiveness, one major limitation remains: the lack of a clear analytical understanding of how these models work. This project aims to explain the internal mechanisms of CNNs by seeking to describe analytically the solutions of these models. We focus on two key areas: diffusion-based generative models and inverse problems in image processing. Our approach is inspired by the work of Ganguli et al. (2024), which provides some of the first analytical interpretations of convolutional neural networks by exploiting the locality and equivariance properties of these models. By using standard and advanced mathematical techniques, including functional analysis, measure theory, and probability, we obtain the analytical solution for the MMSE (minimum mean squared error) denoiser and the analytical solution for the MMSE inverse problem. Our work can be used to assess the performance of convolutional neural networks and predict their outputs.

Keyword: Convolutional network model, locality, equivariance, MMSE denoiser, inverse problem.
\section{Introduction}

In recent years, Convolutional Neural Networks (CNNs) have assumed a pivotal role in the domain of image processing. These methods find application in a variety of tasks, including face generation, image inpainting, and super-resolution, where they frequently yield results that surpass the efficacy of conventional techniques. Despite the efficacy of CNNs in practice, the underlying mechanisms and principles that govern their function remain to not be fully explained. This absence of understanding poses a significant challenge, particularly in the context of seeking reliable and explainable models.

Ganguli et al. (2024) offers one of the first mathematical explanations of CNNs. The paper demonstrated that specific properties inherent to CNNs, such as locality and equivariance are suffiscent to predict the behavior of these models.
\begin{figure}[h] % 'h' means 'here'
    \centering
    \includegraphics[width=0.5\textwidth]{../images/introduction_Ganguli.png} % Change width and filename
    \caption{Predicted images by analytic theory (left columns) and the output of convolutional diffusion model (right coloumns)}
    \label{fig:introduction_im}
\end{figure}

In this project, the objective is to enhance comprehension of CNNs through the utilisation of mathematical tools to elucidate their analytical solutions. The present study focuses on two key areas: diffusion-based generative models utilised for the purpose of image generation, inverse problems concerned with the recovery of original images from noisy data.

The remainder of this paper is structured as follows. Section 2 introduces the core concepts underlying convolutional neural networks (CNNs), score-based diffusion models, and inverse problems, along with the empirical risk minimization framework. Section 3 analyzes analytical solutions for MMSE denoising  without any constraints and with different constraints: ideal, equivariance, localily, and locality and equivariance. Section 4 presents preliminary research findings on MMSE inverse problems under  equivariance and/or locality constraints. Finally, Section 5 concludes with a discussion of implications and future research directions.

\section{Premilinairy}
\subsection{Convolutional neural network (CNN)}
Convolutional Neural Networks (CNNs) are a class of neural architectures grounded in the principles of locality, translation equivariance, and hierarchical composition. CNNs are particularly well-suited for analyzing spatially structured data, such as images, physical fields, or spatiotemporal patterns, due to their ability to exploit the intrinsic structure and symmetries of such data.

The composition of multiple convolutional layers forms a hierarchical representation: early layers extract primitive features (e.g., edges, gradients), which are then recombined by deeper layers into more complex and semantically meaningful structures. This hierarchical organization reflects how many natural and physical systems are structured and enables efficient learning across different spatial scales.
\subsubsection{Network Structure and Hierarchical Representation}
\begin{figure}[h] % 'h' means 'here'
    \centering
    \includegraphics[width=0.5\textwidth]{../images/CNN_illustration.png} % Change width and filename
    \caption{Convolutional neural network illustration}
    \label{fig:CNN_illustration}
\end{figure}
CNNs process input data as multidimensional grids and extract features through the application of convolutional filters, also referred to as kernels. These filters are small, learnable matrices that slide across the input domain to compute local dot products. Each application of a kernel results in a response that highlights the presence of a particular pattern or structure within a localized region. The composition of multiple convolutional layers forms a hierarchical representation: early layers extract primitive features (e.g., edges, gradients), which are then recombined by deeper layers into more complex and semantically meaningful structures. This hierarchical organization reflects how many natural and physical systems are structured and enables efficient learning across different spatial scales.

\subsubsection{Translation Equivariance}
The convolution operation inherently satisfies translation equivariance. This means that when an input is shifted, the output feature map shifts by the same amount, preserving spatial relationships. 
Indeed, given an input $x$, a filter $k$, we denote $y$ the convolution of $x$ and $k$
\begin{equation*}
    y[m,n] = (x*k)[m,n] = \sum_{i,j} x[m-i,n-j]*k[i,j]
\end{equation*}
Let denote $z$ the shifted version of $x$ obtained by shifting $\delta_h$ to the left and $\delta_v$ to the bottom, i.e $z[i,j] = x[i+\delta_h,j+\delta_v]$ the convolution of $z$ and $k$ follows:
\begin{align*}
    (z*k)[m,n] &= \sum_{i,j} z[m-i,n-j]*k[i,j]\\
    &= \sum_{i,j} x[m-i+\delta_h,n-j+\delta_v]*k[i,j]\\
    &= (x*k)[m+\delta_h,n+\delta_v]
\end{align*} 
\subsubsection{Locality}
CNNs incorporate the principle of locality by using filters (kernels) of finite spatial extent. This means each computation depends only on a bounded region of the input, reflecting the assumption that meaningful patterns and interactions are primarily local.
\subsection{Diffusion Model}

\subsubsection{Stochastic differential equations (SDE) and probability flows}

An important part of probalistic modeling is to generate samples from a data distribution whose precise shape is unknown or too complex to allow direct sampling. Diffusion models offer a solution to this problem by learning a time-inhomogeneous differential equation, which gradually transforms samples from a simple Gaussian distribution into samples corresponding to the more complex target distribution.

Consider a time-dependent stochastic differential equation (Itô), given as follows:
    \begin{equation}\label{eq:SDE}
         dx_t = f_t(x_t)dt + g_tdW_t
    \end{equation}
    with 
    \begin{itemize}
        \item $x \in \mathbb{R}^N$
        
        \item $f_t : \mathbb{R}^N \rightarrow \mathbb{R}^N$ is in $C^1$ and is linearly increasing, i.e., $\exists\, C > 0$ such that $\|f_t(x)\| \leq C(1 + \|x\|)$, where $\|\cdot\|$ denotes the $L^2$ norm, i.e., $\forall n \in \mathbb{N}^*, \forall x \in \mathbb{R}^n,\ \|x\| = \sqrt{\sum\limits_{i=1}^n x_i^2}$.
        
        \item $\mathcal{G}_t : \mathbb{R} \rightarrow \mathbb{R}$ is in $C^2$
        
        \item $W_t$ is a standard Wiener process in $\mathbb{R}^N$, with
        \begin{equation*}
            \langle W^i, W^j \rangle_t =
            \begin{cases}
                t & \text{if } i = j \\
                0 & \text{otherwise}
            \end{cases}
        \end{equation*}
        where $W^i$ and $W^j$ are the $i$-th and $j$-th coordinates of $W_t$, respectively.
    \end{itemize}
    
    \begin{remark}
        The condition that $f_t$ is at most linearly increasing guarantees the existence of an SDE solution.
    \end{remark}

\begin{proposition}[Forward Process (Fokker – Planck)\label{prop:fokker}] 
    Under the above assumptions, the flow over the $\pi_t$ probability distributions of $x_t$ for $t \geq 0$ is given by :    \begin{equation}\label{eq:Fokker-Planck}
        \frac{\partial\pi_t}{\partial t} = -\nabla \cdot (f_t \pi_t) + \frac{1}{2}\nabla^2(g_t^2\pi_t)
    \end{equation}
\end{proposition}
\begin{proof}
    The proof is given in \cref{sec:proof_focker}.
\end{proof}
% The condition that $f_t$ is at most linearly increasing guarantees the existence of an EDS solution.
% In general, the direct process is usually constructed so that as $t \rightarrow \infty$ (or $t \rightarrow T$ for some finite time $T$), the distribution $\pi_t$ converges to a known and well-defined distribution $\pi_{\infty}$, often chosen as the normal distribution.
In the context of diffusion models, and more specifically implicit diffusion models for denoising (DDIMs), the objective is to identify a deterministic, time-dependent vector field $v_t$ that exhibits the same flow of probability distributions as the preceding stochastic equation. This reformulation enables the diffusion process to be inverted deterministically:
\begin{itemize}
    \item We begin by sampling $x_T \sim \pi_t$
    \item Then we evolve the sample backwards in time, from $t=T$ to $t = 0$, by solving the following ODE:
    \begin{equation}
        \frac{dx_t}{dt} = v_t(x_t)
    \end{equation}
\end{itemize}
According to the Fokker-Planck equation, the evolution of the $x_t$ 's distribution is given by the following transport equation:
\begin{proposition}[Backward Process\label{prop:fokker}] 
The backward process is giving by the following equation
\[\frac{d\pi_t(x)}{dt} =-\nabla \cdot [v_t(x)\pi_t(x)]\]
\end{proposition}
\begin{proof}
    The above result is a direct application of \cref{eq:Fokker-Planck}.
\end{proof}
Our goal is to identify the deterministic, time-dependent $v_t$ function so that the above equation produces the same evolution as \eqref{prop:fokker} (flow-matching). To do this, we rewrite \eqref{prop:fokker} as follows:
\[\frac{d\pi_t(x)}{dt} =\nabla \cdot([f_t(x)-\frac{1}{2}g_t^2\nabla \log\pi_t(x)]\pi_t(x))\]
Thus,
\[v_t(x) =f_t(x)-\frac{1}{2}g_t^2\nabla \log\pi_t(x) \]
We denote $s_t(x) \equiv \nabla \log\pi_t(x)$, $s_t$ is called a score function. Determining this function plays an important role in recovering the initial image $x_0$.
\subsection{Flow calculation}\label{sec:calcul_numerique}
In practice, the most common choice of direct process for a DDIM is an inhomogeneous Ornstein-Uhlenbeck process. If a positive $(\gamma_t)$ sequence is chosen, it takes the following form:
\begin{equation}\label{eq:OU}
    dx_t = -\gamma_tx_t \,dt + \sqrt{2\gamma_t}\,dW_t
\end{equation}
By identifying in this case $f_t(x) = -\gamma_tx_t$ et $ \mathcal{G}_t = \sqrt{2\gamma_t}$, we deduce the expression for $v_t$ :
\begin{equation}\label{eq:backward}
    \frac{dx_t}{dt} = v_t(x_t) = -\gamma_t(x_t+\nabla \log \pi_t(x_t)) = -\gamma_t(x_t+s_t(x))
\end{equation}
% In this particular instance, it can be demonstrated that $f_t (x_t) = -\gamma_t x_t,  \text{ and } \mathcal{G}_t = \sqrt{2\gamma_t}$. Through these relationships, the expression for $v_t$ can be deduced:
% The expression $v_t (x_t)$ above defines a deterministic vector field, thereby enabling the diffusion process to be reversed by progressively eliminating the noise added during the direct phase. This field integrates the current position, denoted by the symbol $x_t$, and the score field, denoted by the symbol $s_t$, with the score field weighted by a factor $\gamma_t$. The integration of these two fields indicates the direction of maximum probability. In this manner, the quantity $v_t(x_t)$ functions as a guide for the evolution of $x_t$ in a continuous and reversible manner, thereby ensuring that the initially noisy sample $x_T$ is gradually transformed into the original domain data. This ordinary differential equation (ODE) reformulation is imperative for DDIM models, as it facilitates accelerated and optimized generation by circumventing the stochastic simulation of the inverse process.
The direct process given by the equation \eqref{eq:OU} is selected due to the fact that, for any $t \geq 0$, the solution $x_t$ is distributed according to a known distribution, as indicated by the following proposition.
\begin{proposition}\label{prop:solution_processus_direct}
    The solution $x_t$ of \eqref{eq:OU} is given by :
    \begin{equation}\label{eq:solforphi}
        x_t = \sqrt{\bar{\alpha_t}}x_0 + \sqrt{1-\bar{\alpha_t}}\eta
    \end{equation}
    Where $x_0 \sim \pi_0$ is the distribution of the initial images to be sampled, $\eta$ is an isotropic Gaussian vector, i.e. $\eta \sim \mathcal{N}(0, I_N)$, and $\bar{\alpha_t}$ is defined by 
    \[\bar{\alpha_t} = \exp{\left(-2\int_0^t \gamma_s ds\right)}.\]    
\end{proposition}
\begin{proof}
    The proof is detailed in \cref{sec:proof_solution_processus_direct}
\end{proof}
In particular, we implicitly choose $\gamma_t$ so that $\bar \alpha_t \rightarrow 0$ when t tends towards $\infty$, the distribution tends to reduced-centered normal distribution and no longer depends on the initial  distribution $\pi_0$ of $x_0$. The time-dependent parameter $\bar \alpha_t$ is called the “noise schedule”, as it controls the speed of convergence $\pi_t$. In practice, $\bar \alpha_t \rightarrow 0$ when t tends towards a certain finite time T. In this way, we can sample $x_T$ a centered reduced normal distribution, then recover $x_0$ thanks to the inverse process we'll see next.

% The fundamental premise of the diffusion model hinges on the reversal of the direct process, thereby facilitating the sampling of the target distribution, denoted by $p_0$, through the inverse process. The latter requires knowledge of the score function $s_t = -d/dlog(p_t)$. The initial step in this process is the calculation of the probability distribution of the random variable $x_t$, denoted by $\pi_t$.

% The following two results are essentially useful for calculating the analytical expression of $\pi_t$.
\begin{proposition}\label{prop:quelques_resultats_sur_la_densite}
    Let X and Y be two independent random variables in $\R^n$, $f_X$ and $f_Y$ are the density functions of $X$ and $Y$ respectively. We obtain the following properties:
    \vspace{-10pt}
\begin{enumerate}[label=(\roman*)]
    \item Let $\alpha \in \R_+^*$, and $U = \alpha X$, then the density fonction of $U$ is given by $\forall x\in \R^n$, $f_U(x) = \frac{1}{\alpha^n} f_X(\frac{x}{\alpha})$.
    \item Let $Z = X+Y$, The density of $Z$ is the convolution of the the density funciton of $X$ and the density function of $Y$, i.e $f_Z(x) = (f_X * f_Y)(x)$
\end{enumerate}
\end{proposition}

\begin{proof}
The proof of these results is detailed in appendix \cref{sec:proof_quelques_resultates_sur_la_densite}.
\end{proof}


% The objective is to express the density of the target state, denoted by the symbol $\pi_t$, in terms of the transformations undergone by the probe state, denoted by the symbol $x_t$, during the direct process. Two above results on the densities of random variables are employed: firstly, the linear transformation of a random variable, which enables the deduction of the effect of a scaling factor on the density; and secondly, the convolution of densities, which facilitates the calculation of the density of the sum of two independent random variables. By employing the aforementioned properties, it is possible to express the probability of the occurrence of the phenomenon under consideration, denoted here by $\pi_t$, as an integral of the initial density $\pi_0$, convolved with a normal distribution of covariance \((1 - \bar{\alpha}_t) I_N\). This reflects the progressive diffusion of the initial distribution under the effect of Gaussian noise. To be more precise, the distribution of the random variable $t$ is given by the following proposition.
\begin{proposition}\label{prop:distribution_a_etap_t}
    Let the direct process presented by \cref{eq:solforphi}, under the assumption of the existence of the $\pi_t$ distribution of $x_t$, The distribution $\pi_t$ is given by :    \begin{equation}\label{eq:distribution_pi_t}
        \pi_t(x) = \int_{\R^N} \pi_0(z)\, \mathcal{G}_t(x - \sqrt{\bar \alpha_t}\,z) dz
    \end{equation}
    with $\mathcal{G}_t$ the density fonction of a normal distribution$\mathcal{N}\left(0_{\R^N}, (1- \alpha _t \,)I_N\right)$
\end{proposition}

\begin{proof}
    See \cref{sec:proof_distribution_a_etap_t} for a full demonstration.
\end{proof}
% The analytical expression of $\pi_t$ shows that the direct process applies a progressive smoothing to the initial distribution $\pi_0$, convolving it with a normal distribution. Under the assumption that $\bar \alpha_t \rightarrow \infty$ when $t$ tends towards $1$ (or towards some large $T$), the larger $t$ is, the further $x_t$ is from its initial state and tends towards an isotropic normal distribution.
We derive the score function from $\pi_t$ :
\begin{align*}
        s_t(x) &= \nabla \log \pi_t(x) = \frac{1}{\pi_t(x)}\nabla\pi_t(x) \\
        &= \frac{1}{\pi_t(x)}\nabla\int_{\R^N} \pi_0(z) g(x - \sqrt{\bar \alpha_t} \,z) dz\\
\end{align*}
Let's denote $u: \R^N \times \R^N \rightarrow \R$ the quantity inside the above integral defined by :
\[u(x,z) =  \pi_0(z) \mathcal{G}_t(x - \sqrt{\bar \alpha_t}\,z), \quad \forall (x,z) \in \R^N \times \R^N\]
The function $u$ is continuous on $\R^N \times \R^N$ and integrable with respect to $z$. Furthermore, $\nabla_x u$ exists, is continuous on $\R^N \times \R^N$ and is integrable with respect to $z$. Then, using Leibniz's integral rule, we obtain :
\begin{equation}\label{eq:score_formule_premilinaire}
    s_t(x) = \frac{1}{\pi_t(x)} \int_{\R^N} \pi_0(z) \nabla_x \mathcal{G}_t(x - \sqrt{\bar \alpha_t} \,z) dz 
\end{equation}
Since $\mathcal{G}_t$ is the normal law density function $\mathcal{N}\left(0_{\R^N}, (1-\sqrt{\bar \alpha _t},)I_N\right)$, by definition, the function $\mathcal{G}_t$ is written :
\begin{equation*}
    \mathcal{G}_t(x) = \frac{1}{\sqrt{2\pi}(1-\bar \alpha_t)^{\frac{N}{2}}} \, \exp\left(-\frac{1}{2(1-\bar \alpha _t)}\,x^Tx\right) \quad \forall x \in \R^N
\end{equation*}
Thus
\begin{equation*}
    \nabla_x \,\mathcal{G}_t(x) = -\frac{1}{1-\bar \alpha_t}g(x)x
\end{equation*}
Thus
\begin{equation*}
    \nabla_x \,\mathcal{G}_t(x - \sqrt{\bar \alpha_t} \,z) = -\frac{1}{1-\bar \alpha_t}\,\mathcal{G}_t(x - \sqrt{\bar \alpha_t} \,z)(x - \sqrt{\bar \alpha_t} \,z)
\end{equation*}
Plugging this result into \eqref{eq:score_formule_premilinaire} :
\begin{equation}\label{eq:score_formule_analytique}
    s_t(x) = -\frac{1}{1- \bar \alpha_t} \int_{\R^N} \frac{\pi_0(z)\, \mathcal{G}_t(x - \sqrt{\bar \alpha_t} \,z)(x - \sqrt{\bar \alpha_t} \,z)}{\pi_t(x)} dz
\end{equation}
% The equation \eqref{eq:score_formule_analytique} expresses the score function \(s_t\), which represents the logarithmic gradient of the density \(\pi_t\). This function indicates the direction in which \(\pi_t\) must be adjusted to recover a probable initial state. The integral shows that \(s_t(x)\) is obtained by averaging over all possible initial values \(z\), weighted by their original probability \(\pi_0(z)\) and the noise added during diffusion. The term \((x - \sqrt{\bar{\alpha}_t}\, z)\) reflects the correction needed to reverse the diffusion, and normalization by \(\pi_t(x)\) ensures that this correction is local. Finally, the factor \(-\frac{1}{1- \bar \alpha_t}\) adjusts the intensity of this correction according to the noise level. This result is fundamental to scattering models, as it enables us to estimate the optimal direction in which to remove the added noise, thus generating realistic samples from initial noise.
The intervention of the target distribution $\pi_0$ makes the score function $s_t$ implicit. However, there's an extremely useful fact about this score function that we can take advantage of to learn it from the data by using the Tweedie's formula.
\begin{theorem}[Tweedie's formula]\label{theo:formule_de_Tweedie}
    Let $X \in \mathbb{R}^n$ be a random variable of density $f_X$ and $B \sim \mathcal{N}(0, \sigma^2 I_n)$ a Gaussian noise independent of $X$. We define :
    \[ Y = X + B \]
    Let's denote $\varphi_B$ and $p_Y$ the density function of $B$ and $Y$ respectively. We obtain :
    \[ \mathbb{E}[X | Y = y] = y + \sigma^2 \nabla \log p_Y(y) \]
\end{theorem}

\begin{proof}
    The proof given in \cref{sec:proof_formule_de_Tweedie}
\end{proof}
Let recall the direct process
\[ x_t = \sqrt{\bar \alpha_t} x_0 + \sqrt{1 - \bar \alpha_t} \eta \]
Using the  Tweedie formula, we derive the score function :
\begin{align*}
s_t(x) &= \nabla \log \pi_t(x) \\
&= \frac{-x + \mathbb{E} \left[\sqrt{\bar \alpha_t}\, x_0 \mid x_t = x \right]}{1 - \bar \alpha_t} \\
&= \frac{1}{1 - \bar \alpha_t} \left( -x + \sqrt{\bar \alpha_t} \,\mathbb{E} \left[ x_0 \mid x_t = x \right] \right)
\end{align*}
This result is extremely useful, as it allows us to numerically approximate the score function $s_t$, which is proportional to $\mathbb{E}[x_0 | x_t]$. Indeed, by definition of conditional expectation, $\mathbb{E}[x_0 | x_t]$ is the orthogonal projection of $x_0$ onto the functional space of $x_t$. This relationship can be represented mathematically as :
\[
\mathbb{E}[x_0 | x_t] = \argmin_{f \in \mathbb{L}^2(\mathbb{R}^N)} \mathbb{E}\left[ \| x_0 - f(x_t) \|^2 \right], \quad t \in [0,T]
\]
% Ainsi, la fonction de score $s_t = \frac{-1}{(\alpha_t)^{N/2} \sqrt{1-\bar{\alpha}_t}} \mathbb{E}\left[ \eta - f(x_t) \right]$ est définie comme :
% \[
% s_t = \argmin_{f \in \mathbb{L}^2(\mathbb{R}^N)} \mathbb{E}\left[ \left\| \frac{1}{(\bar \alpha_t)^{N/2} \sqrt{1-\bar{\alpha}_t}} \eta + f(x_t) \right\|_2^2 \right], \quad t \in [0,T]
% \]
% We want to approximate $s_t$ for all $t \in [0,T]$, not just on a given $t$. This idea motivates us to build a neural network that models a function $f_z(x,t)$ by minimizing the following cost function:
% \[
% \mathcal{L}(z) = \mathbb{E}_{t \sim U([0,T]), x_t \sim \pi_t, x_0 \sim \pi_0} \left[ \| f_z(x_t,t) - x_ 0\|^2 \right]
% \]
% The optimal function $f_z$ can be approximated numerically by solving the associated regression problem. Once $f_z$ has been acquired, the score function $s_t$ can be obtained directly by :
% \[
% s_t(x)= \frac{1}{1 - \bar \alpha_t} \left( -x + \sqrt{\bar \alpha_t} \, f_z(x,t) \right)
% \]

\subsection{Inverse Problem}
Inverse problems aim to recover a target variable $x \in \R^N$ from a noisy mesure $y \in \R^M$, typically modeled as
\begin{equation*}
    y = Ax +b, \text{ with } A:\R^N \rightarrow \R^N
\end{equation*}
A modern approach is to learn a function $f : y \rightarrow \hat{x}$ using a convolutional neural network trained on a supervised dataset ${(x_i,y_i)}_{i=1}^\mathcal{D}$. The network is trained by minimizing the empirical risk associated with a loss function
\begin{equation*}
    \min_{f} \E{f(y)-x}
\end{equation*}
    

\subsection{Conclusion}
In the context of both difusion models and inverse problems, uderstanding how a neural network works requires us to analyze empirical risk minnimizers. Surprisingly, we will see that that locality and equivariance constraints aone are sufficient to predict the network's behavior.

\section{Analytical solution for the MMSE denoising}
In this section, our work is to understand and reconstruct the proofs to better grasp the cencepts \todo{of what}.
\subsection{Ideal Score (IS) machine}
We will show in this section that a diffusion model that learns the ideal score function on a finite set of data can only memorize and cannot create new samples far from the training data.

We do not have access to the target distribution $\pi_0$ because we have a finite number of samples, so we estimate it by the empirical discrete distribution on the training data set $\mathcal{D}$ :
\begin{equation*}
    \pi_0(x) = \frac{1}{|\mathcal{D}|} \sum\limits_{x_0  \in \mathcal{D}} \delta(x - \varphi)
\end{equation*}
With $|\mathcal{D}|$ the number of elements in $\mathcal{D}$ and $\delta$ a Dirac mass.En utilisant \cref{eq:distribution_pi_t}, à l'étape temporelle $t$, on obtient la distribution des images bruitées :
\begin{align*}
\pi_t(x) &= \int_{\mathbb{R}^N} \pi_0(z) \mathcal{G}_t(x - \sqrt{\bar \alpha_t}z) dz \quad \text{with } \mathcal{G}_t \text{ is the density function of normal distribution } \mathcal{N}(0, (1-\bar \alpha_t)I_N) \\
&= \frac{1}{|\mathcal{D}|} \sum\limits_{x_0  \in \mathcal{D}} \int_{\mathbb{R}^N} \delta(z - x_0) \mathcal{G}_t(x - \sqrt{\bar \alpha_t}z) dz \\
&= \frac{1}{|\mathcal{D}|} \sum\limits_{x_0  \in \mathcal{D}} \mathcal{G}_t(x - \sqrt{\bar \alpha_t}x_0)
\end{align*}

The estimated score function $s_t = \nabla \log \pi_t$, which gives us 
:\begin{align*}
s_t(x) &= \nabla \log \pi_t(x) = -\sum\limits_{x_0  \in \mathcal{D}} (x - \sqrt{\bar \alpha_t}x_0) \frac{\mathcal{G}_t(x - \sqrt{\bar \alpha_t}x_0)}{\pi_t(x)} \\
&= -\frac{1}{1-\bar \alpha_t} \sum\limits_{x_0  \in \mathcal{D}} (x - \sqrt{\bar \alpha_t}x_0) W_t(x_0 | x) \\
& \text{with }W_t(x_0| x) = \frac{\mathcal{G}_t(x - \sqrt{ \bar \alpha_t}x_0)}{\sum\limits_{x_0  \in \mathcal{D}} \mathcal{G}_t(x - \sqrt{\bar \alpha_t}x_0')}
\end{align*}
% In general, this process involves averaging the added noise. This is done by taking the noise vectors $\eta \propto (x - \sqrt{\bar \alpha_t}x_0)$ between the observed example $x$ and each training element $\varphi$, with weights based on the probability $W(\varphi|x)$ that the noisy image $x_t$ at time step $t$ would come from a point $\varphi$ of the training set $\mathcal{D}$ . This probability is calculated using Bayes' theorem: it is the probability that a training example $\varphi$ will give the observed example $x$, as a function of the amount of noise required to transform $\varphi$ into $x$, compared with all other possible examples $\varphi'$. The weights $W(\varphi|x)$ are calculated using a simple softmax function applied to a quadratic loss function $-\frac{1}{2(1-\bar \alpha_t)}\||x - \sqrt{\bar \alpha_t}x_0|^2$ for each $\varphi$ point in the training set.
Our aim is to show that learning the ideal score function on the finite data set can only memorize, meaning that $x_t$ converge to an element $x_0 \in \mathcal{D}$ as $t \rightarrow 0$. We first recall the inverse process:
\begin{equation*}
    \frac{dx}{dt} =  -\gamma_t(x+s_t(x)), \quad \text{avec } \gamma_t = -\frac{\partial \bar \alpha_t}{2\bar \alpha_t} 
\end{equation*}
% Lors du processus inverse, le score idéal agit comme un guide dynamique : il calcule, pour chaque sample \(x_t\), des forces proportionnelles à la proximité de \(x_t\) avec chaque donnée atténuée $\sqrt{\bar \alpha_t} \varphi$, pondérées par des probabilités \emph{a posteriori} (\(W_t(\varphi|x)\)). Ce score crée un effet d’amplification : si le sample est proche d’une donnée réduite, le modèle devient plus "sûr" que c’est la bonne direction, et le tire encore plus vers elle. Cette certitude augmente progressivement, forçant le sample à converger vers exactement la même donnée d’entraînement.
% Il est important de noter que la fonction de score idéale ne correspond pas vraiment aux modèles de diffusion réels. Elle a tendance à mémoriser les données d'entraînement, surtout lorsqu'on travaille avec des données à haute dimension, car les points d'entraînement sont très éloignés les uns des autres. Cela nécessite beaucoup plus de données pour bien couvrir l'espace sous-jacent afin que la fonction de score empirique puisse bien approximer la vraie fonction de score idéale pour toutes les entrées et à tous les moments.
% Le fait que la fonction de score idéale ne soit pas un bon modèle pour les processus de diffusion réalistes nous montre qu'il faut comprendre pourquoi ces modèles ne résolvent pas parfaitement leurs tâches. Il faut donc examiner les biais et contraintes qui empêchent ces modèles d'apprendre la fonction de score idéale et voir comment ils se comportent malgré ces limitations.
Assume that the limits \( \lim_{t \to 0} x_t \) and \( \lim_{t \to 0} \partial_t x_t \) exist, which implies that
\[
\lim_{t \to 0} \gamma_t s_t(x_t) \text{ also exist.}
\]
One has
\begin{equation*}
    \lim\limits_{t \to 0} \gamma_t s_t (x_t) = \lim\limits_{t \to 0} \frac{\partial \bar{\alpha}_t}{2\bar{\alpha}_t} \frac{1}{1 - \bar{\alpha}_t} \sum\limits_{\varphi \in D} \left( x_t - \sqrt{\bar{\alpha}_t} \varphi \right) W(\varphi | x_t)
\end{equation*}
As  $\lim\limits_{t \to 0} \bar{\alpha}_t = 1$, the prefactor diverges. This means:
\begin{equation*}
    \lim\limits_{t \to 0} \sum\limits_{\varphi \in D} (x_t - \sqrt{\bar{\alpha}_t} \varphi) W(\varphi | x_t) = 0
\end{equation*}
we note that $\mathcal{N}(x_t | \sqrt{\bar{\alpha}_t} \varphi, (1 - \bar{\alpha}_t) I_N)$ tends towards a dirac function when $t \to 0$ because $\bar{\alpha}_t \xrightarrow[]{t \to 0} 1$ . Thus
\begin{equation*}
    \lim\limits_{t \to 0} W(\varphi | x_t) =
\begin{cases} 
1 & \text{if } \varphi = \varphi^* \coloneq \arg\min_{\varphi \in D} ||x_t - \varphi||^2 \\
0 & \text{else}
\end{cases}
\end{equation*}
Indeed
\begin{equation*}
    W(\varphi | x_t) =
\frac{\exp\left( -\frac{||x_t - \sqrt{\bar{\alpha}_t} \varphi||^2}{(1 - \bar{\alpha}_t)^N} \right)}
{\sum\limits_{\varphi' \in D} \exp\left( -\frac{||x_t - \sqrt{\bar{\alpha}_t} \varphi'||^2}{(1 - \bar{\alpha}_t)^N} \right)}
=\frac{1}{\sum\limits_{\varphi' \in D} \exp\left( \frac{||x_t - \sqrt{\bar{\alpha}_t} \varphi'||^2 - ||x_t - \sqrt{\bar{\alpha}_t} \varphi||^2}{(1 - \bar{\alpha}_t)^N} \right)}
\end{equation*}
Let $A(x_t, \varphi, \varphi' ) = \exp\left( \frac{||x_t - \sqrt{\bar{\alpha}_t} \varphi'||^2 - ||x_t - \sqrt{\bar{\alpha}_t} \varphi||^2}{(1 - \bar{\alpha}_t)^N} \right)$, we note that
\begin{equation*}
    \lim\limits_{t \to 0} A(x_t, \varphi, \varphi^*) =
\begin{cases} 
+\infty & \text{if } \varphi \neq \varphi^* \\
1 & \text{if } \varphi = \varphi^*
\end{cases}
\end{equation*}
In addition
\begin{equation*}
    \lim\limits_{t \to 0} A(x_t, \varphi^*, \varphi') = 0 \quad \forall \varphi' \in D \setminus \{\varphi^*\}
\end{equation*}
Thus
\[
\lim\limits_{t \to 0} W(\varphi | x_t) =
\begin{cases} 
1 & \text{if } \varphi = \varphi^* \\
0 & \text{else}
\end{cases}
\]
We obtain
\[
0 = \lim\limits_{t \to 0} \sum\limits_{\varphi \in D} (x_t - \sqrt{\bar{\alpha}_t} \varphi) W(\varphi | x_t) = \lim\limits_{t \to 0} (x_t - \varphi^*)
\]
\begin{equation*}
    \lim\limits_{t\rightarrow 0} x_t = \varphi^* \in \mathcal{D}
\end{equation*}

In conclusion, at date $t = 0$, the sample tends towards an image in the training set $\mathcal{D}$. The fact that the ideal score function is not a good model for realistic diffusion processes shows us that we need to understand why these models don't solve their tasks perfectly. We therefore need to examine the biases and constraints that prevent these models from learning the ideal score function, and see how they perform despite these limitations.
\section{Score function estimator under constraints}
\subsection{General}
\subsubsection{Problem}
We would like to solve
\begin{equation*}
    \min_{f\in\mathcal{M}} \E{\| f(t,x)-s_t(x)}
\end{equation*}
for some vector space $\mathcal{M}$
\subsubsection{Optimal condition}
$f^* \in \mathcal{M}$ is a minimiser of $J$ if $J(f^*+\epsilon h) \geq J(f^*)$ for all $\epsilon \in \R$ and all $h\in \mathcal{M}$. We have
\begin{align*}
    J(f^*+\epsilon h) &= \E{\|(f^*+\epsilon h)(t,x)-s_t(x)\|^2}\\
    & = J(f^*)+\epsilon^2\E{\|h(t,x)\|^2}+2\epsilon\E{\langle h(t,x),f*(t,x) - s_t(x) \rangle}
\end{align*}
therefore, the optimal condition
\begin{equation}\label{eq:optimal_condition_score_function}
    \E{\langle h(t,x),f*(t,x) - s_t(x) \rangle} = 0 \text{ for all } x \in \mathcal{M}
\end{equation}

\subsection{Equivairant (ES) machine}
\subsubsection{Group of transformation}
\begin{definition}[Group of transformation]
    A \textbf{group of transformations} is a set \( G \) of applications of a set \( X \) into itself, with a composition operation of transformations that satisfies the following axioms:
    \begin{itemize}[topsep=-5pt]
    \item \textbf{Fermeture} : \( \forall U, V \in G, \quad U \circ V \in G \).
    \item \textbf{Associativité} : \( (U \circ V) \circ W = U \circ (V \circ W) \).
    \item \textbf{Identité} : \( \exists e \in G \) so that \( e \circ U = U \circ e = U \).
    \item \textbf{Inverses} : \( \forall U \in G, \quad \exists !\,U^{-1} \in G \text{ tel que } U \circ U^{-1} = U^{-1} \circ U = e \).
\end{itemize}
\end{definition}



For example, the symmetry group in $\R^2$ is the set of transformations that preserve the Euclidean distance. More precisely, it includes rotation and reflection:
\begin{itemize}
    \item A rotation of angle $z$ about the origin is given by the rotation matrix :\[
R_z =
\begin{bmatrix}
\cos z & -\sin z \\
\sin z & \cos z
\end{bmatrix}.
\]

\item a reflection about any axis in \( \mathbb{R}^2 \) can be represented by a matrix of the form :
\[
S_\varphi =
\begin{bmatrix}
\cos \varphi & \sin \varphi \\
\sin \varphi & -\cos \varphi
\end{bmatrix}.
\]
This retains the standard, but reverses the direction.
\end{itemize}

\begin{definition}
    A transformation group $G$ acting on a complex vector space $X$ provided with the scalar product $\langle \cdot,\cdot \rangle$ is said to be unitary if for any transformation $U \in G$, we have :
    \begin{equation*}
        \langle Ux,Uy\rangle = \langle x,y\rangle \quad\forall(x,y) \in X^2
    \end{equation*}
\end{definition}


Transformations in a unitary group preserve geometric properties such as distances and angles between vectors, but only in complex vector spaces. The example of the $\R^2$ symmetry group above is also a unitary group.
\begin{proposition}\label{prop:Groupe_unitaire}
    Let G be a group of unitary transformations acting on a complex vector space X. Let $U$ be an element in $G$, and we have the following properties:
        \begin{enumerate}[label=(\roman*)]
        \item \textbf{Inverse et adjoint} : $\forall U \in G$, $U^{-1} = U^\dagger$ avec $U^\dagger$ l'adjoint de $U$.
        \item \textbf{Préservation de norme} : $\|Ux\| = \|x\| \quad \forall x\in X$.
        \item \textbf{Transformation de gradient} : pour toute fonction $f \in C^1(X,\R)$, le gradient se transforme comme :
        \begin{equation*}
            \nabla_x f(U^{-1}x) = U\, \nabla f(U^{-1}x) \quad \forall x\in X.
        \end{equation*}
    \end{enumerate}
\end{proposition}

\begin{proof}
    The proof is in '\cref{sec:proof_groupe_unitaire}
\end{proof}
\subsubsection{Analytical expression for Equivariance score (ES) machine}
% L'équivariance dans les modèles de diffusion garantit que le processus génératif respecte les symétries naturelles des données, ce qui est essentiel pour de nombreuses applications. En intégrant ces symétries dans l'architecture du modèle, on préserve la cohérence structurelle des échantillons générés, tout en réduisant la redondance dans l'apprentissage et en améliorant la généralisation. Cela permet également une optimisation plus stable et évite l’introduction de dépendances arbitraires qui pourraient nuire à la performance du modèle. On va étudier dans cette section comment un modèle de diffusion apprend la fonction de score sous la contrainte d'équivariance.
The admissible set of function $\mathcal{M}$ is defined by
\begin{equation*}
    \mathcal{M} = {f:[0,T]\times \R^N} \rightarrow \R^N \text{ such that } f(t,T_g x) = T_g f(t,x), \quad \forall g\in G,\forall x\in \R^N
\end{equation*}
Plugging this estimator into the optimal condition \eqref{eq:optimal_condition_score_function}
\begin{equation*}
\E{\langle h(t,x),f^*(t,x) -s_t(x)\rangle} = 0
\end{equation*}
One has
\begin{align*}
    & \E{\langle h(t,x),f^*(t,x) -s_t(x)\rangle} \\
    &= \frac{1}{|\mathcal{D}|}\sum_{x_0  \in \mathcal{D}} \int_{\R^N} \langle h(t,x),f^*(t,x)-s_t(x) \rangle \mathcal{G}_t(x -\sqrt{\bar \alpha_t}x_0)dx\\
    &= \frac{1}{|\mathcal{D}|}\sum_{x_0  \in \mathcal{D}} \int_{\R^N} \int_\mathcal{G }\langle h(t,x),f^*(t,x) \mathcal{G}_t(x -\sqrt{\bar \alpha_t}x_0)-s_t(x) \mathcal{G}_t(x -\sqrt{\bar \alpha_t}x_0) \rangle dx\\
    &= \frac{1}{|\mathcal{D}|} \int_{\R^N} \int_G\langle h(t,x),\sum_{x_0  \in \mathcal{D}}f^*(t,x) \mathcal{G}_t(x -\sqrt{\bar \alpha_t}x_0)-\sum_{x_0  \in \mathcal{D}} \frac{-1}{1-\bar \alpha_t} (x -\sqrt{\bar \alpha_t})\mathcal{G}_t(x -\sqrt{\bar \alpha_t}x_0) \rangle dx\\
    &= \frac{1}{|\mathcal{D}|} \int_{\R^N} \int_G\langle h(t,x),\sum_{x_0  \in \mathcal{D}}f^*(t,x) \mathcal{G}_t(T_g(x -\sqrt{\bar \alpha_t}x_0))-\sum_{x_0  \in \mathcal{D}} \frac{-1}{1-\bar \alpha_t} (x -\sqrt{\bar \alpha_t})\mathcal{G}_t(T_g(x -\sqrt{\bar \alpha_t}x_0)) \rangle dx dT_g\\
    &= \frac{1}{|\mathcal{D}|} \int_{\R^N} \int_G\langle h(t,x),\sum_{x_0  \in \mathcal{D}}f^*(t,x) \mathcal{N}(T_gx,T_g \sqrt{\bar \alpha_t}x_0,(1-\bar\alpha_t)I_N)-\sum_{x_0  \in \mathcal{D}} \frac{-1}{1-\bar \alpha_t} (x -\sqrt{\bar \alpha_t})\mathcal{N}(T_gx,T_g \sqrt{\bar \alpha_t}x_0,(1-\bar\alpha_t)I_N) \rangle dx dT_g\\
    &= \frac{1}{|\mathcal{D}|} \int_{\R^N} \int_G\langle T_g^{-1}h(t,T_gx),\sum_{x_0  \in \mathcal{D}}T_g^{-1}f^*(t,T_gx) \mathcal{N}(T_gx,T_g \sqrt{\bar \alpha_t}x_0,(1-\bar\alpha_t)I_N)-\sum_{x_0  \in \mathcal{D}} \frac{-1}{1-\bar \alpha_t} T_g^{-1}(T_gx -T_g \sqrt{\bar \alpha_t})\mathcal{N}(T_gx,T_g \sqrt{\bar \alpha_t}x_0,(1-\bar\alpha_t)I_N) \rangle dx dT_g\\
    &= \frac{1}{|\mathcal{D}|} \int_{\R^N} \int_G\langle T_g^{-1}h(t,T_gx),\sum_{x_0  \in \mathcal{D}}T_g^{-1}f^*(t,T_gx) \mathcal{N}(T_gx,T_g \sqrt{\bar \alpha_t}x_0,(1-\bar\alpha_t)I_N)-\sum_{x_0  \in \mathcal{D}} \frac{-1}{1-\bar \alpha_t} T_g^{-1}(T_gx -T_g \sqrt{\bar \alpha_t})\mathcal{N}(T_gx,T_g \sqrt{\bar \alpha_t}x_0,(1-\bar\alpha_t)I_N) \rangle dx dT_g\\
    &= \frac{1}{|\mathcal{D}|} \int_{\R^N} \int_G\langle h(t,T_gx),\sum_{x_0  \in \mathcal{D}}f^*(t,T_gx) \mathcal{N}(T_gx,T_g \sqrt{\bar \alpha_t}x_0,(1-\bar\alpha_t)I_N)-\sum_{x_0  \in \mathcal{D}} \frac{-1}{1-\bar \alpha_t} (T_gx -T_g \sqrt{\bar \alpha_t})\mathcal{N}(T_gx,T_g \sqrt{\bar \alpha_t}x_0,(1-\bar\alpha_t)I_N) \rangle dx dT_g\\
    &= \frac{1}{|\mathcal{D}|} \int_{\R^N} \int_G\langle h(t,x),\sum_{x_0  \in \mathcal{D}}f^*(t,x) \mathcal{N}(x,T_g \sqrt{\bar \alpha_t}x_0,(1-\bar\alpha_t)I_N)-\sum_{x_0  \in \mathcal{D}} \frac{-1}{1-\bar \alpha_t} (x -T_g \sqrt{\bar \alpha_t})\mathcal{N}(x,T_g \sqrt{\bar \alpha_t}x_0,(1-\bar\alpha_t)I_N) \rangle dx dT_g\\
    &= \frac{1}{|\mathcal{D}|} \int_{\R^N} \langle h(t,x),\int_G \sum_{x_0  \in \mathcal{D}}f^*(t,x) \mathcal{N}(x,T_g \sqrt{\bar \alpha_t}x_0,(1-\bar\alpha_t)I_N)-\sum_{x_0  \in \mathcal{D}} \frac{-1}{1-\bar \alpha_t} \int_G(x -T_g \sqrt{\bar \alpha_t})\mathcal{N}(x,T_g \sqrt{\bar \alpha_t}x_0,(1-\bar\alpha_t)I_N) \rangle dx dT_g\\
\end{align*}
The optimality condition implies that the above quantity is zero $\forall h \in \mathcal{M}$, thus
\begin{equation*}
    \sum_{x_0  \in \mathcal{D}}\int_G f^*(t,x) \mathcal{N}(x,T_g \sqrt{\bar \alpha_t}x_0,(1-\bar\alpha_t)I_N)-\sum_{x_0  \in \mathcal{D}} \int_G \frac{-1}{1-\bar \alpha_t} (x -T_g \sqrt{\bar \alpha_t})\mathcal{N}(x,T_g \sqrt{\bar \alpha_t}x_0,(1-\bar\alpha_t)I_N)  =0
\end{equation*}
Thus
\begin{equation*}
    f^*(t,x) =\frac{-1}{1-\bar \alpha_t}\frac{\sum_{x_0  \in \mathcal{D}}\int_G  (x -T_g \sqrt{\bar \alpha_t})\mathcal{N}(x,T_g \sqrt{\bar \alpha_t}x_0,(1-\bar\alpha_t)I_N) dT_g}{\sum_{x_0  \in \mathcal{D}}\int_G \mathcal{N}(x,T_g \sqrt{\bar \alpha_t}x_0,(1-\bar\alpha_t)I_N) d T_g}
\end{equation*}
The conditions on the transformation $T_g$ are important: We need to do a change of variable $T_g x \rightarrow x$ and pass the adjoint $T_g^{-1}$ to the other side of the inner product. The following properties were used:
\begin{itemize}
    \item $|\det(T_g T_g^\top)| = 1$
    \item $(T_g^{-1})^\top=T_g $
\end{itemize}
In the image case, $G$ corresponds to every possible spatial translation, which is indeed a group of unitary transformations. We also note that $G(\mathcal{D})$ is finite, since $\mathcal{D}$ contains a finite number of images and each image has a finite number of pixels. Consequently, integrals over $G$ in the above expression convert to sums over $G(\varphi)$ 
\begin{align*}
        f^*(t,x) &= -\frac{1}{1-\bar \alpha_t} \sum\limits_{x_0  \in \mathcal{D}} \sum\limits_{\varphi' \in G(\varphi)} (x - \sqrt{\bar \alpha_t} \varphi') \frac{\mathcal{N}(x | \sqrt{\bar \alpha_t} \varphi', (1-\bar \alpha_t) I_N) }{\sum\limits_{x_0  \in \mathcal{D}} \sum\limits_{\varphi'\in G(\varphi)} \mathcal{N}(x | \sqrt{\bar \alpha_t} \varphi', (1-\bar \alpha_t) I_N)}\\
        &= -\frac{1}{1-\bar \alpha_t}  \sum\limits_{\varphi' \in G(\mathcal{D})} (x - \sqrt{\bar \alpha_t} \varphi') \frac{\mathcal{N}(x | \sqrt{\bar \alpha_t} \varphi', (1-\bar \alpha_t) I_N) }{ \sum\limits_{\varphi'\in G(\mathcal{D})} \mathcal{N}(x | \sqrt{\bar \alpha_t} \varphi', (1-\bar \alpha_t) I_N)}
\end{align*}
% \begin{definition}
% Soit \( G \) un groupe particulier de transformations agissant sur les données \( x \). On dit qu'un modèle \( M_t \) est \( G \)-équivariant si, pour tout \( U \in G \), notre modèle satisfait
% \vspace{-5pt}
% \begin{equation*}
%     M_t[Ux] = UM_t[x].
% \end{equation*}
% \end{definition}
% Cela signifie que l'application de la transformation $U$ à l'entrée$x$, suivie du passage à travers le modèle$M_t$, produit le même résultat que l'application du modèle $M_t$ à $x$, suivie de la transformation $U$ sur la sortie. En particulier, on cherche l'expression analytique de $M_t$ sous un groupe de transformation unitaire, le résultat est donné par le théorème suivant.
% \begin{theorem}\label{theo:equivariant_modele}
% Soit $G$ un groupe de transformation unitaire, l'approximation \( G \)-équivariante optimale de la fonction de score empirique sous l'objectif de score matching est donnée par la fonction de score empirique pour l'ensemble de données \( G(\mathcal{D}) \) consistant en l'orbite de l'ensemble de données \( \mathcal{D} \) sous le groupe \( G \), i.e 
% \begin{equation}\label{eq:model_sous_equivariant_contraint}
%     M_t(\psi) = -\frac{1}{1-\bar \alpha_t} \frac{\sum\limits_{x_0  \in \mathcal{D}} \int_{G(\varphi)} (\psi - \sqrt{\bar \alpha_t} \varphi') \mathcal{N}(\psi | \sqrt{\bar \alpha_t} \varphi', (1-\bar \alpha_t) I) d\varphi'}{\sum\limits_{x_0  \in \mathcal{D}} \int_{G(\varphi)} \mathcal{N}(\psi | \sqrt{\bar \alpha_t} \varphi', (1-\bar \alpha_t) I) d\varphi'} \quad ,\forall\psi \in \R^N
% \end{equation}
% \end{theorem}
% \begin{proof}
%     Le proof est détaillé dans l'\cref{sec:proof_equivariant_modele}
% \end{proof}
% Dans le cas d'image, $G$ correspond à toute translation spatiale possible, qui est bien un groupe de transformations unitaires. On constate également que $G(\mathcal{D})$ est fini, car $\mathcal{D}$ contient un nombre fini d'images et chaque image possède un nombre fini de pixels. Par conséquent, les intégrales sur $G(\varphi)$ dans l'expression de $M_t$ se convertissent en sommes sur $G(\varphi)$ :
% \begin{align*}
%         M_t(\psi) &= -\frac{1}{1-\bar \alpha_t} \sum\limits_{x_0  \in \mathcal{D}} \sum\limits_{\varphi' \in G(\varphi)} (\psi - \sqrt{\bar \alpha_t} \varphi') \frac{\mathcal{N}(\psi | \sqrt{\bar \alpha_t} \varphi', (1-\bar \alpha_t) I) }{\sum\limits_{x_0  \in \mathcal{D}} \sum\limits_{\varphi'\in G(\varphi)} \mathcal{N}(\psi | \sqrt{\bar \alpha_t} \varphi', (1-\bar \alpha_t) I)}\\
%         &= -\frac{1}{1-\bar \alpha_t}  \sum\limits_{\varphi' \in G(\mathcal{D})} (\psi - \sqrt{\bar \alpha_t} \varphi') \frac{\mathcal{N}(\psi | \sqrt{\bar \alpha_t} \varphi', (1-\bar \alpha_t) I) }{ \sum\limits_{\varphi'\in G(\mathcal{D})} \mathcal{N}(\psi | \sqrt{\bar \alpha_t} \varphi', (1-\bar \alpha_t) I)}
% \end{align*}
% La sortie de la machine ES est quasiment identique à celle de l'apprentissage idéal de la fonction de score, sauf qu'ici l'ensemble de données $\mathcal{D}$ est augmenté par les translations spatiales de $G$. Ce résultat montre que la machine ES appliquée aux images n'atteint qu'une créativité limitée : elle ne peut générer que des translations des images dans l'ensemble d'entraînement.
\subsection{Local score machine}
In this section, we will study the behavior of the score function estimator under the locality constraint.
The admissible set of functions is the set of functions that act locally, i.e 
\begin{equation*}
    \mathcal{M} = {f:[0,T]\times\R^N \rightarrow \R^N \text{ such that } \forall n, \exists v_{f,n} : \R^P \rightarrow \R, f_n(t,x)=v_{f,n}(t,\Sigma_n x, \forall x\in \R^N)}
\end{equation*}
Where $\Sigma_n \in \R{P\times N}$ is a selection matrix, which take $P$ pixels centered at $n$ (with e.g., circular boundary condition). The function $f = (f_1,f_2,\dots,f_N)$ have the output $f_n(x)$ depend only on $\Sigma_n x$, a small patch centered at n with the aciton $v_{f,n}$.

At some point, we need to do a change of variables so it's helpful to look at the section matrices 
$\Sigma_n : \mathbb{R}^N \rightarrow \mathbb{R}^P$. 
Since it selects $P$ pixels from the input image at some location, it has the general form:
\[
\Sigma = 
\begin{pmatrix}
e_{\sigma_1} \\
\vdots \\
e_{\sigma_P}
\end{pmatrix}
\in \mathbb{R}^{P \times N}
\]
where $e_{\sigma_1}, \ldots, e_{\sigma_P}$ are distinct elements from the canonical basis of $\mathbb{R}^N$. 
It's obvious that $\Sigma$ is full row rank and
\[
\Sigma \Sigma^T = I_P \qquad \text{since} \qquad (\Sigma \Sigma^T)_{i,j} = \langle e_{\sigma_i}, e_{\sigma_j} \rangle = \delta_{i,j}.
\]
It's important to note that the above properties hold true for any patch center $n$. 
Moreover, the (Moore–Penrose) pseudo-inverse of $\Sigma$ is simply $\Sigma^T$.

One has
\begin{align*}
    \E{\langle h(t,x),f^*(t,x) -s_t(x)\rangle} &= \int_{\R^N} \langle h(t,x),f^*(t,x)-s_t(x) \rangle \pi_t(x) dx\\
    &= \int_{\R^N} \sum_{n=1}^{N}\langle v_{h,n}(t,\Sigma_nx),v_{f^*,n}(t,\Sigma_nx)-s_t(x) \rangle \pi_t(x) dx\\
\end{align*}
We fix $n \in {1,2,\dots,N}$. We choose $(v_{h,n})_n$ defined by
\begin{equation*}
    v_{h,j}(t,\Sigma_n x) = 
    \begin{cases}
        0 & \text{if } j = n \\
        \delta(\Sigma_nx - b) & \text{if } j \neq n
    \end{cases}
    ,\quad x \in \R^P
\end{equation*}
Thus
\begin{equation*}
    \int_{\R^N} \sum_{n=1}^{N}\langle v_{h,n}(t,\Sigma_nx),v_{f^*,n}(t,\Sigma_nx)-s_t(x)[n] \rangle \pi_t(x) dx =  \int_{\R^N} \delta(\Sigma_nx -b)\,(v_{f^*,n}(t,\Sigma_nx)-s_t(x)[n])  \pi_t(x) dx
\end{equation*}
According to the optimality condition, the alove quantity is zero, thus
\begin{align*}
    v_{f^*,n}(t,b)\int_{\R^N} \delta(\Sigma_nx -b) \pi_t(x) dx &= \int_{\R^N} \delta(\Sigma_nx -b) s_t(x)[n]\pi_t(x) dx \\
    &= \int_{\R^N} \delta(\Sigma_nx -b) \nabla_{x[n]}\pi_t(x) dx\\
    &= \nabla_{b[0]} \int_{\R^N} \delta(\Sigma_nx -b) \pi_t(x) dx\\
\end{align*}
We consider
\begin{align*}
    \int_{\R^N} \delta(\Sigma_nx -b) \pi_t(x) dx &= \frac{1}{|\mathcal{D}|} \sum_{x_0  \in \mathcal{D}} \int_{\R^N} \delta(\Sigma_nx -b) \Normal{x; \sqrt{\bar \alpha_t} x_0, (1-\bar\alpha_t) \Id_N} dx\\
    & \eqdef \frac{1}{|\mathcal{D}|} \sum_{x_0  \in \mathcal{D}} q(b, n, \sqrt{\bar\alpha_t}x_0)
\end{align*}
We need to compute the weights $q(b, n, \sqrt{\bar\alpha_t}x_0)$ for any $x \in \R^P$, $b\in \R^P$, $n \in [1, \dots, N]$, one has
\begin{align*}
    q(b, n, \sqrt{\bar\alpha_t}x_0)&=\int_{\R^N} \delta(\Sigma_nx -b) \Normal{x; \sqrt{\bar \alpha_t} x_0, (1-\bar\alpha_t) \Id_N} dx\\
    &= \int_{\R^P} \delta(z -b) \Normal{z; \sqrt{\bar \alpha_t} \Sigma_n x_0, (1-\bar\alpha_t) \Id_R} dz\\
    &= \Normal{b; \sqrt{\bar \alpha_t} \Sigma_n x_0, (1-\bar\alpha_t) \Id_R}
\end{align*}
\begin{proposition}[Normal integration on subspace]\label{propr:normal_on_subspace}
    Let $\mathcal{V} = v_0 + \mathcal{V'} \subset \R^N$ be a $(N - K)$ dimensional affine subspace of $\R^N$. Suppose that  $V$ is an orthonormal basis of $\mathcal{V'}$. For any $\mu \in \R^N$, we have:
    \begin{equation*}
        \int_{\mathcal{V}} \Normal{w; \mu, \sigma^2 \Id_{N}} d \mathcal{H}^{N - K}(w) = \frac{1}{(2 \pi \sigma^2)^{(K / 2)}} \exp \left( -\frac{\norm{(\Id - VV^T)(\mu - v_0)}^2}{2\sigma^2} \right)
    \end{equation*}
\end{proposition}
\begin{proof}
    The proof is in \cref{sec:proof_normal_on_subspace}
\end{proof}

% \begin{align*}
%     q(b, n, \sqrt{\bar\alpha_t}x_0) &= \int_{\R^N} \delta(\Sigma_nx -b) \Normal{x; \sqrt{\bar \alpha_t} x_0, (1-\bar\alpha_t) \Id_N} dx\\
%     &= \int_{v_n^{-1}(b)} \Normal{w; \sqrt{\bar \alpha_t }x_0, (1-\bar\alpha_t) \Id_N} d \H^{N - P}(w) 
% \end{align*}
% Firstly, we look at the level-set $v_n^{-1}(x)$ for any $z \in \R^P$:
% \begin{equation*}
%     v_n^{-1}(x) = \{ x \in \R^N: \Sigma_n x = b \} = \Sigma_n^+ b + \ker{\Sigma_n}  = \Sigma_n^T b + \ker{\Sigma_n}  
% \end{equation*}
% Since $\Sigma_n$ is full row rank, $v_n^{-1}(b)$ is an affine subspace of $\R^N$ of dimension $\dim(\ker{\Sigma_n}) = N - P$.  

% Let $V_n$ is an orthonormal basis of $ker(\Sigma_n)$. Applying \cref{propr:normal_on_subspace}, we have:
% \begin{equation*}
%     q(b, n, \sqrt{\bar\alpha_t}x_0) = \frac{1}{(2\pi (1-\bar\alpha_t)^2)^{P/2}} \exp \left( -\frac{\norm{(\Id - V_n V_n^T)(\sqrt{\bar\alpha_t}x_0 - \Sigma_n^T b)}^2}{2(1-\bar\alpha_t)} \right)
% \end{equation*}
% Since $ V_nV_n^T$ is the projection onto $\ker{\Sigma_n}^\perp = \Im{\Sigma_n^T}$ and $\Sigma_n^T b \in \Im{\Sigma_n^T}$, we can further simplify as:
% \begin{equation*}
%     q(b, n, \sqrt{\bar\alpha_t}x_0) = \frac{1}{(2\pi (1-\bar\alpha_t))^{P/2}} \exp \left( -\frac{\norm{(\Id - V_n V_n^T)\sqrt{\bar\alpha_t}x_0 - \Sigma_n^T b}^2}{2(1-\bar\alpha_t)} \right)
% \end{equation*}
Thus,
\begin{align*}
    \nabla_{b[0]}q(b, n, \sqrt{\bar\alpha_t}x_0) &= \frac{-1}{1-\bar\alpha_t} \left(b[0] -  \sqrt{\bar{\alpha_t}}x_0[n]\right) q(b, n, \sqrt{\bar\alpha_t}x_0)\\
    &= \frac{-1}{1-\bar\alpha_t} \left(b[0] -  \sqrt{\bar{\alpha_t}}x_0[n]\right) \Normal{b; \sqrt{\bar \alpha_t} \Sigma_n x_0, (1-\bar\alpha_t) \Id_R}
\end{align*}
We obtain finally the analytical expression for the LS machine
\begin{equation}
    v_{f^*,n}(t,b) = -\frac{1}{1-\bar\alpha_t} \frac{\sum_{x_0\in\mathcal{D}}\left(b[0] -  \sqrt{\bar{\alpha_t}}x_0[n]\right) \Normal{b; \sqrt{\bar \alpha_t} \Sigma_n x_0, (1-\bar\alpha_t) \Id_R}}{\sum_{x_0 \in \mathcal{D}} \Normal{b; \sqrt{\bar \alpha_t} \Sigma_n x_0, (1-\bar\alpha_t) \Id_R}}
\end{equation}
% The final step is to verify if the above expression of LS machine satisfies the optimality condition \eqref{eq:optimal_condition_score_function}
% \begin{align*}
%     &\E{\langle h(t,x),f^*(t,x) -s_t(x)\rangle}\\
%     &= \int_{\R^N} \langle h(t,x),f^*(t,x)-s_t(x) \rangle \pi_t(x) dx\\
%     &= \sum_{n=1}^{N} \int_{\R^N} \langle v_{h,n}(t,\Sigma_nx),v_{f^*,n}(t,\Sigma_nx)-s_t(x) \rangle \pi_t(x) dx\\
%     &= \sum_{n=1}^{N} \int_{\R^N} v_{h,n}(t,\Sigma_nx)\left(\sum_{x_0 \in \mathcal{D}}  v_{f^*,n}(t,\Sigma_nx)\mathcal{G}_t(x-\sqrt{\bar\alpha_t}x_0)-\frac{-1}{1-\bar\alpha_t}\sum_{x_0\in\mathcal{D}}(x-\sqrt{\bar{\alpha_t}}x_0) \mathcal{G}_t(x-\sqrt{\bar\alpha_t}x_0)\right)dx\\
%     &= \sum_{n=1}^{N} \int_{\R^P} \int_{v_n^{-1}(b)} v_{h,n}(t,\Sigma_n \omega)\left(\sum_{x_0 \in \mathcal{D}}  v_{f^*,n}(t,\Sigma_n\omega)\mathcal{G}_t(\omega-\sqrt{\bar\alpha_t}x_0)-\frac{-1}{1-\bar\alpha_t}\sum_{x_0\in\mathcal{D}}(\omega-\sqrt{\bar{\alpha_t}}x_0) \mathcal{G}_t(\omega-\sqrt{\bar\alpha_t}x_0)\right)d\H^{N-P}(\omega) db\\
%     &= \sum_{n=1}^{N} \int_{\R^P}  v_{h,n}(t,z)\left(\sum_{x_0 \in \mathcal{D}}  v_{f^*,n}(t,z)\int_{v_n^{-1}(b)}\mathcal{G}_t(\omega-\sqrt{\bar\alpha_t}x_0)d\H^{N-P}(\omega)-\frac{-1}{1-\bar\alpha_t}\sum_{x_0\in\mathcal{D}}\int_{v_n^{-1}(b)}(\omega-\sqrt{\bar{\alpha_t}}x_0) \mathcal{G}_t(\omega-\sqrt{\bar\alpha_t}x_0)d\H^{N-P}(\omega)\right) db\\
% \end{align*}




% La propriété de localité s'exprime par le fait que la sortie de $M_t$ sur le pixel se situant en $x$ ne dépend pas de pixels en dehors du voisinage local $\Omega_x$. Le modèle local $M_t$ peut s'écrire comme suit:
% \[ M_t[x](x) = f_x[x_{\Omega_x}] \]

% Le problème d'identifier le modèle local optimal revient à chercher l'optimal fonctionnel qui minimise la fonction de coût suivante :
% \[ \mathcal{L}(f_x) = \mathbb{E}_{x \sim \pi_t} \left[ \| f_x[x_{\Omega_x}] - s_t[x](x) \|^2 \right] \]

% En écrivant l'espérance sous forme d'intégrale,
% \[ \mathcal{L}(f_x) = \int_{\mathbb{R}^N} \pi_t(x) \| f_x(x_{\Omega_x}) - s_t[x](x) \|^2 dx \]

% On introduit une perturbation fonctionnelle $h$ à $\mathcal{L}$, soit $\varepsilon > 0$,
% \[ \mathcal{L}(f_x + \varepsilon h) = \mathcal{L}(f_x) + 2\varepsilon \int_{\mathbb{R}^N} \pi_t(x) \langle h(x_{\Omega_x}), f_x(x_{\Omega_x}) - s_t[x](x) \rangle dx + o(\varepsilon^2) \]

% $f_x$ est l'optimal fonctionnel de $\mathcal{L}$ si
% \[ \frac{d\mathcal{L}}{d\varepsilon}(f_x) = 0 \quad \forall h \]

% Ainsi,
% \[ \int_{\mathbb{R}^N} \pi_t(x) (f_x(x_{\Omega_x}) - s_t[x](x)) h(x_{\Omega_x}) dx = 0 \]

% On choisit $h$ de sorte que $h(x_{\Omega_x}) = \delta(x_{\Omega_x} - x)$ avec $x$ arbitraire point de meme dimension que $x_{\Omega x}$, ce qui nous donne :
% \[
% \int_{\mathbb{R}^N} \pi_t (x) \left[ \partial_x (x_{\Omega_x} - x) - \partial_t \left[ x_t(x) \right] \right] \delta(x_{\Omega_x} - x) \, dx = 0
% \]

% En arrangeant :
% \[
% \int_{\mathbb{R}^N} \pi_t (x) f_x (x_{\Omega_x} - x) \delta(x_{\Omega_x} - x) \, dx
% = \int_{\mathbb{R}^N} \pi_t (x) s_t[x](x)\delta(x_{\Omega_x} - x) \, dx
% \]

% Le terme à gauche s'écrit :
% \begin{align*}
% \int_{\mathbb{R}^N} \pi_t(x) f_x(x_{\Omega_x}) \delta(x_{\Omega_x} - x) dx &= f_x(x) \int_{\mathbb{R}^N} \pi_t(x) \delta(x_{\Omega_x} - x) dx \\
% &= f_x(x) p_{x_t \sim \pi_t} (x_{t \Omega_x} = x)\\
% & \text{avec } p_{x_t \sim \pi_t} (x_{t \Omega_x} = x) \text{ la densité marginale de }x_{t \Omega_x}
% \end{align*}

% Let denote $p(t,x,x)=\int_{\mathbb{R}^N} \pi_t(x) \delta(x_{\Omega_x} - x) dx$. We need to compute the weight $p(t,x,x)$ for any $t\in [0,T]$, any pixel location $x$, and for any $x \in \R^P$:
% \begin{equation*}
%     p(t,x,x)=\int_{\mathbb{R}^N} \pi_t(x) \delta(x_{\Omega_x} - x) dx
% \end{equation*}



% On manipule le terme à droite :
% \begin{align*}
% \int_{\mathbb{R}^N} \pi_t(x) s_t[x](x) \delta(x_{\Omega_x} - x) dx &= \int_{\mathbb{R}^N} \delta(x_{\Omega_x} - x) \pi_t(x) \nabla_{x(x)} \log \pi_t(x) dx \\
% &= \int_{\mathbb{R}^N} \delta(x_{\Omega_x} - x) \nabla_{x(x)} \pi_t(x) dx \\
% &= \nabla_{x(0)} p_{x_t \sim \pi_t} (x_{t \Omega_x} = x)
% \end{align*}

% Injectons dans l'équation :
% \[ f_x(x)\, p_{x_t \sim \pi_t} (x_{t \Omega_x} = x) = \nabla_{x(0)} p_{x_t \sim \pi_t} (x_{t \Omega_x} = x) \]

% D'où
% \[ f_x(x) = \nabla_{x(0)} \log p_{x_t \sim \pi_t} (x_{t\Omega_x} = x) \]

% On a :
% \begin{align*}
% \mathbb{P}_{x_t \sim \pi_t} (x_{t, \Omega_x} = x) &= \int_{\mathbb{R}^N} \pi_t(x) \delta(x_{\Omega_x} = x) dx \\
% &= \frac{1}{|\mathcal{D}|} \sum\limits_{x_0  \in \mathcal{D}} \int_{\mathbb{R}^N} \mathcal{G}_t(x - \sqrt{\bar \alpha_t} \varphi) \delta(x_{\Omega_x} - x) dx \\
% &= \frac{1}{|\mathcal{D}|} \sum\limits_{x_0  \in \mathcal{D}} \mathcal{G}_t(x, \sqrt{\bar \alpha_t} \varphi_{\Omega_x} + (1 - \bar \alpha_t) I)
% \end{align*}

% Ainsi,
% \begin{align*}
%     f_x(x) &= \nabla_{x(0)} \log \left\{p_{x_t \sim \pi_t} (x_{t\Omega_x} = x)\right\} \\
%     &=\nabla_{x(0)} \log \left\{ \frac{1}{|\mathcal{D}|} \sum\limits_{x_0  \in \mathcal{D}} \mathcal{N}(x \mid \sqrt{\bar \alpha_t} \varphi_{\Omega_x}, (1 - \bar\alpha_t) I) \right\}
% \end{align*}

% D'où
% \begin{align*}
% M_t[x](x) &= f(x_{\Omega_x}) = \nabla_{x(x)} \left\{ \sum\limits_{x_0  \in \mathcal{D}} \mathcal{N}(x_{\Omega_x} \mid \sqrt{\bar \alpha_t} \varphi_{\Omega_x}, (1 - \bar \alpha_t) I) \right\} \\
% &= -\frac{1}{1 - \bar \alpha_t} \sum\limits_{x_0  \in \mathcal{D}} \left(x(x) - \sqrt{\bar \alpha_t} \varphi(x)\right) \frac{\mathcal{N}(x_{\Omega_x} \mid \sqrt{\bar \alpha_t} \varphi_{\Omega_x}, (1 - \bar \alpha_t) I)}{\sum\limits_{x_0  \in \mathcal{D}} \mathcal{N}(x_{\Omega_x} \mid \sqrt{\bar \alpha_t} \varphi'_{\Omega_x}, (1 - \bar \alpha_t) I)}
% \end{align*} 

% La LS machine donne une sortie qui est quasiment identique à celle de l'apprentissage idéal de la fonction de score, sauf que l'ensemble de données $\mathcal{D}$ est remplacé par l'ensemble de local patches issus de $\mathcal{D}$. Intuitivement, un échantillon $x$ généré par la LS machine respecte trois conditions locales :
% \begin{itemize}[topsep=-5pt]
%     \item Chaque pixel $x$ peut être attribué de manière unique à un patch d'apprentissage local $\varphi_x$ situé en $x$.
%     \item La valeur du pixel $x(x)$ est exactement égale au pixel central $\varphi_x(0)$.
%     \item Le reste du patch local généré $x_{\Omega_x}$ ressemble, au sens $l_2$, davantage au patch d'entraînement local $f$ qu'à tout autre patch d'entraînement possible.
% \end{itemize}

% Ce résultat montre que la local score (LS) machine approxime la fonction score en se basant sur des patchs d’image locaux. Cela permet à différentes régions \( x_{\Omega_x} \) et \( x_{\Omega_{x'}} \) de s’aligner sur des patchs issus d’images d’entraînement distinctes, offrant ainsi une plus grande créativité combinatoire. Toutefois, une limitation persiste : chaque patch ne peut correspondre qu’à un patch d’entraînement situé au même emplacement spatial. L’ajout de l’équivariance lève cette contrainte, permettant aux patchs de se transférer entre différentes positions et d’accroître encore la créativité.

\subsection{Local and translational equivariant score function}
In this section, we will study the behavior of the score function estimator under the locality and translational equivariance constraints.
The admissible set of functions is the set of functions that act locally and equivariant by tranlsation, i.e 
\begin{equation*}
    \mathcal{M} = {f:[0,T]\times\R^N \rightarrow \R^N \text{ such that }  \exists v_{f} : \R^P \rightarrow \R, f_n(t,x)=v_{f}(t,\Sigma_n x, \forall x\in \R^N)}
\end{equation*}

One has
\begin{align*}
    \E{\langle h(t,x),f^*(t,x) -s_t(x)\rangle} &= \int_{\R^N} \langle h(t,x),f^*(t,x)-s_t(x) \rangle \pi_t(x) dx\\
    &= \int_{\R^N} \sum_{n=1}^{N} v_{h}(t,\Sigma_nx)\left(v_{f^*}(t,\Sigma_nx)-s_t(x) \right) \pi_t(x) dx\\
\end{align*}
We choose $(v_{h})$ defined by
\begin{equation*}
    v_{h}(t,\Sigma_n x) = \delta(\Sigma_n x - z)
\end{equation*}
Thus
\begin{equation*}
    \int_{\R^N} \sum_{n=1}^{N}\langle v_{h}(t,\Sigma_nx),v_{f^*}(t,\Sigma_nx)-s_t(x)[n] \rangle \pi_t(x) dx = \sum_{n=1}^{N} \int_{\R^N} \delta(\Sigma_nx -b)\,(v_{f^*}(t,\Sigma_nx)-s_t(x)[n])  \pi_t(x) dx
\end{equation*}
According to the optimality condition, the alove quantity is zero, thus
\begin{align*}
    \int_{\R^N} \sum_{n=1}^{N}\delta(\Sigma_nx -b) \pi_t(x) dx &= \sum_{n=1}^{N}\int_{\R^N} \delta(\Sigma_nx -b) s_t(x)[n]\pi_t(x) dx \\
    &= \sum_{n=1}^{N}\int_{\R^N} \delta(\Sigma_nx -b) \nabla_{x[n]}\pi_t(x) dx\\
    &= \sum_{n=1}^{N}\nabla_{b[0]} \int_{\R^N} \delta(\Sigma_nx -b) \pi_t(x) dx\\
\end{align*}

Thus
\begin{align*}
    v_{f^*}(t,b) \sum_{n=1}^{N}\sum_{x_0  \in \mathcal{D}} q(b, n, \sqrt{\bar\alpha_t}x_0) &= \nabla_{b[0]} \sum_{n=1}^{N}\sum_{x_0  \in \mathcal{D}} q(b, n, \sqrt{\bar\alpha_t}x_0)\\
    v_{f^*}(t,b) \sum_{n=1}^{N}\sum_{x_0  \in \mathcal{D}} \Normal{b; \sqrt{\bar \alpha_t} \Sigma_n x_0, (1-\bar\alpha_t) \Id_R} &=  \sum_{n=1}^{N}\sum_{x_0  \in \mathcal{D}} \left(b[0] -  \sqrt{\bar{\alpha_t}}x_0[n]\right) \Normal{b; \sqrt{\bar \alpha_t} \Sigma_n x_0, (1-\bar\alpha_t) \Id_R}\\
\end{align*}

We obtain the analytic formula for the local in translational equivariant MMSE
\begin{equation*}
    v_{f^*}(t,b) = -\frac{1}{1-\bar\alpha_t}\,\frac{\sum_{n=1}^{N}\sum_{x_0  \in \mathcal{D}} \left(b[0] -  \sqrt{\bar{\alpha_t}}x_0[n]\right) \Normal{b; \sqrt{\bar \alpha_t} \Sigma_n x_0, (1-\bar\alpha_t) \Id_R}}{\sum_{n=1}^{N}\sum_{x_0  \in \mathcal{D}} \Normal{b; \sqrt{\bar \alpha_t} \Sigma_n x_0, (1-\bar\alpha_t) \Id_R}}
\end{equation*}

% \subsection{Optimal score function}
% Dans cette section, nous allons intégrer les propriétés de localité et d'équivariance dans notre machine qui estime la fonction de score. On l'appelle \textbf{équivariant local machine} (ELM), notée $M_t$.

% On peut modéliser notre ELM par :
% \begin{equation*}
%     M_t[x](x) = f \left[ x_{\Omega_x} \right]
% \end{equation*}

% La fonction $f$ est optimisée sous la fonction de coût suivante :
% \begin{equation*}
%     \mathcal{L}(f) = \sum\limits_x \mathbb{E}_{x_t \sim \pi_t} \left( \| f(x_{t\Omega_x}) - s_t [x_t](x) \|^2 \right)
% \end{equation*}

% On écrit la fonction de coût sous forme d'integrale,
% \begin{equation*}
%     = \int_{\R^N} \pi_t(x) \sum\limits_x \| f(x_{\Omega_x}) - s_t [x](x) \|^2 dx
% \end{equation*}

% Comme d'habitude, on introduit une perturbation fonctionnelle $h$ à $\mathcal{L}$, soit $\varepsilon > 0$,
% \begin{align*}
%     \mathcal{L}(f + \varepsilon h) &= \int_{\R^N} \pi_t(x) \sum\limits_x \| f(x_{\Omega_x}) + \varepsilon h(x_{\Omega_x}) - s_t [x](x) \|^2 dx\\
%     &= \mathcal{L}(f) + 2\varepsilon \int_{\R^N} \pi_t(x) \sum\limits_x \langle h(x_{\Omega_x}), f(x_{\Omega_x}) - s_t [x](x) \rangle dx + O(\varepsilon^2)
% \end{align*}

% $f$ est minimiseur de $\mathcal{L}$ si :
% \begin{equation*}
%     \frac{d}{d\varepsilon} \mathcal{L}(f) = 0 \quad \forall h
% \end{equation*}

% C'est-à-dire :
% \begin{equation*}
%     \int_{\R^N} \pi_t(x) \sum\limits_x \left( f(x_{\Omega_x}) - s_t [x](x) \right) h(x_{\Omega_x}) dx = 0
% \end{equation*}

% On choisit $h$ de sorte que $h(x_{\Omega_x}) = \delta(x_{\Omega_x} - x)$ avec $x$ arbitraire lot. Ce qui nous donne
% \begin{equation*}
%     \int_{\R^N} \pi_t(x) \sum\limits_x \left( f(x_{\Omega_x}) - s_t [x](x) \right) \delta(x_{\Omega_x} - x) dx = 0
% \end{equation*}

% En arrangeant,
% \begin{align*}
%     \int_{\R^N} \pi_t(x) \sum\limits_x s_t [x](x) \delta(x_{\Omega_x} - x) dx &= \int_{\R^N} \pi_t(x) \sum\limits_x f(x_{\Omega_x}) \delta(x_{\Omega_x} - x) dx \\
%     &= f(x) \sum\limits_x p_{x_t \sim \pi_t} (x_{t\Omega_x} = x)\\
%     & \text{avec }  p_{x_t \sim \pi_t} (x_{t\Omega_x} = x) \text{ la densité marginale de }x_{t\Omega_x}
% \end{align*}

% Le terme à gauche s’écrit :
% \begin{align*}
%     \int_{\mathbb{R}^N} \pi_t(x) \sum\limits_x s_t[x](x) \delta(x_{\Omega_x} - x) dx &= \sum\limits_x \int_{\mathbb{R}^N} \delta(x_{\Omega_x} - x) \pi_t(x) \nabla_{x_(x)} \log \pi_t(x) dx\\
%     &= \sum\limits_x \int_{\mathbb{R}^N} \delta(x_{\Omega_x} - x) \nabla_{x_(x)}\pi_t(x) dx\\
%     &= \sum\limits_x \nabla_{x(0)} p_{x_t \sim \pi_t} \left(x_{t\Omega_x} = x \right)
% \end{align*}



% Ainsi,
% \begin{equation*}
%     f(x) = \nabla_{x(0)} \log \sum\limits_x p_{x_t \sim \pi_t} \left(x_{t\Omega_x} = x \right)
% \end{equation*}

% On rappelle la distribution \(\pi_t\) :
% \begin{align*}
%         \pi_t(x) &= \frac{1}{|\mathcal{D}|} \sum\limits_{x_0  \in \mathcal{D}} \mathcal{G}_t (x - \sqrt{\bar \alpha_t} \varphi)\\
%         & \text{avec \(\mathcal{G}_t \) la fonction de densité de la loi normale $\mathcal{N}(0, (1 - \bar \alpha_t) I_N)$ }:
% \end{align*}


% On cherche :
% \begin{align*}
%     p_{x_t \sim \pi_t} \left( x_{t\Omega_x} = x \right) &=
% \int_{\mathbb{R}^N} \pi_t(x) \delta(x_{\Omega_x} - x) dx \\
% & = \frac{1}{|\mathcal{D}|} \sum\limits_{x_0  \in \mathcal{D}} \int_{\mathbb{R}^N} \mathcal{G}_t (x - \sqrt{\bar \alpha_t} \varphi) \delta(x_{\Omega_x} - x) dx\\
% &= \frac{1}{|\mathcal{D}|} \sum\limits_{x_0  \in \mathcal{D}} \int_{\mathbb{R}^N} \mathcal{N} (x | \sqrt{\bar \alpha_t} \varphi, (1 - \bar \alpha_t) I)\, \delta(x_{\Omega_x} - x) dx\\
% &= \frac{1}{|\mathcal{D}|} \sum\limits_{x_0  \in \mathcal{D}} \mathcal{N} (x | \sqrt{\bar \alpha_t} \varphi_{\Omega_x}, (1 - \bar \alpha_t) I)
% \end{align*}

% D’où :
% \begin{align*}
%     \sum\limits_x p_{x_t \sim \pi_t} \left( x_{t\Omega_x} = x \right) &= \frac{1}{|\mathcal{D}|} \sum\limits_x \sum\limits_{x_0  \in \mathcal{D}} \mathcal{N} (x | \sqrt{\bar \alpha_t} \varphi_{\Omega_x}, (1 - \bar \alpha_t) I)\\
%     &=\frac{1}{|\mathcal{D}|} \sum\limits_{\varphi \in P_\Omega(\mathcal{D})} \mathcal{N} (x | \sqrt{\bar \alpha_t} \varphi_{\Omega_x}, (1 - \bar \alpha_t) I)
% \end{align*}

% Injectons ce résultat dans l’expression de l’optimale fonctionnelle \( f_t \) :
% \begin{align*}
% f_t(x_{\Omega_x}) &= \nabla_{x(x)} \left\{ \sum\limits_{\varphi \in P_\Omega(\mathcal{D})} \mathcal{N} (x_{\Omega_x} | \sqrt{\bar \alpha_t} \varphi, (1 - \bar \alpha_t) I) \right\}\\
% &= -\frac{1}{1 - \bar \alpha_t}\sum\limits_{\varphi \in P_\Omega(\mathcal{D})}  \left(x(x) - \sqrt{\bar \alpha_t} \varphi(0)\right)\, W(\varphi | x_{\Omega_x})\\
% & \text{avec } W(\varphi | x_{\Omega_x}) = \frac{\mathcal{N} (x_{\Omega_x} | \sqrt{\bar \alpha_t} \varphi, (1 - \bar \alpha_t) I)}
% {\sum\limits_{\varphi' \in P_\Omega(\mathcal{D})} \mathcal{N} (x_{\Omega_x} | \sqrt{\bar \alpha_t} \varphi', (1 - \bar \alpha_t) I)}
% \end{align*}

% En pratique, on utilise souvent zero-paddings dans un réseau CNN. On adapte notre ELS machine de sorte qu'elle peut prendre en compte les pixels dont le $\Omega$-voisinage dépasse la frontière de l'image. Plus précisément, on remplace l'ensemble de tout patch $P_\Omega(\mathcal{D})$ par les dictionnaires de patchs $P_\Omega^x(\mathcal{D})$ dependant de $x$ :
% \begin{align*}
%     M_t(x)[x] &=  -\frac{1}{1 - \bar \alpha_t}\sum\limits_{\varphi \in P_\Omega^x(\mathcal{D})}  \left(x(x) - \sqrt{\bar \alpha_t} \varphi(0)\right)\, W(\varphi | x_{\Omega_x},x)\\
%     & \text{avec } W(\varphi | x_{\Omega_x},x) = \frac{\mathcal{N} (x_{\Omega_x} | \sqrt{\bar \alpha_t} \varphi, (1 - \bar \alpha_t) I)}
%     {\sum\limits_{\varphi' \in P_\Omega^x(\mathcal{D})} \mathcal{N} (x_{\Omega_x} | \sqrt{\bar \alpha_t} \varphi', (1 - \bar \alpha_t) I)}
% \end{align*}

% En présence de zero-paddings, différents dictionnaires de patchs de l'ensemble d'apprentissage sont utilisés pour le calcul de la machine ELS en fonction de l'information contextuelle fournie par la bordure visible à l'intérieur du patch. Pour un patch central sans information sur le bord, les patchs de l'ensemble d'apprentissage proviennent de l'ensemble de l'intérieur de l'image. Pour les patchs de bord, les patchs de l'ensemble d'apprentissage proviennent de tous les bords de l'image à la même distance de la bordure. Pour les patchs d'angle, seules les patchs provenant de cet endroit précis sont utilisées dans le calcul.

\subsubsection{Convergence at $t = 0$}
\begin{definition}[Point localment cohérent]
    Dans le contexte de ELS machine, un point \(\hat{x}\) est dit localement cohérent si 
    pour tout pixel \( x \), la valeur \(\hat{x}(x)\) est égale au pixel central \( \varphi(0) \) avec :
\[
    \varphi = \arg \min_{\varphi \in P_{\Omega}^{x}} \|\Psi - \hat{x}_{\Omega_t}^{x}\|^2.
\]
\end{definition}


\begin{theorem}
    On se donne un échantillon \(x_T\) issu de la distribution \(\pi_T\) et on évolue cet échantillon d'après le processus inverse :
    \[
    \partial_t x_t = - \gamma_t \left( x_t + M_t(x_t) \right) \quad \text{avec} \quad \gamma_t = -\frac{\partial_t \bar{\alpha}_t}{2 \bar \alpha_t}.
    \]
    Sous l’hypothèse que les limites \( \lim\limits_{t \to 0} x_t \) et \( \lim\limits_{t \to 0} \partial_t x_t \) existent, l’échantillon \( x_t \) converge vers un point localement cohérent.
\end{theorem}


\begin{proof}
    L’hypothèse de l’existence de \( \lim\limits_{t \to 0} \partial_t \bar \alpha_t \) et de \( \lim\limits_{t \to 0} x_t \) entraîne que \( \gamma_t M_t(x_t) \) reste borné lorsque \( t \to 0 \).

    On a :
    \[
    \lim\limits_{t \to 0} M_t [x_t] (x) = \lim\limits_{t \to 0} - \frac{\partial_t \bar \alpha_t}{2\bar  \alpha_t (1 - \bar \alpha_t)} \sum_{\varphi \in P_{\Omega}^{x}} \left(x_t(x) - \sqrt{\bar{\alpha}_t} \varphi(0)\right) W(\varphi | x_t, x).
    \]
    
    Avec :
    \[
    W(\varphi | x_t, x) = \frac{\mathcal{N}(x_{t\Omega_x} | \sqrt{\bar{\alpha}_t} \varphi, (1 - \alpha_t)I)}
    {\sum\limits_{\varphi \in P_{\Omega}^{x}} \mathcal{N}(x_{t\Omega_x} | \sqrt{\bar{\alpha}_t} \varphi', (1 - \alpha_t)I)}.
    \]

    Comme $
    \lim\limits_{t \to 0} \frac{1}{1 - \bar \alpha_t} = +\infty 
    \text{ et } 
    \frac{\partial_t \bar \alpha_t}{2 \bar \alpha_t} = \gamma_t \text{ existe et continue en } t=0,$
    la somme à droite doit tendre vers 0 lorsque \( t \to 0 \), c'est-à-dire :    
    \[
    \lim\limits_{t \to 0} \sum_{\varphi \in P_{\Omega}^{x}} \left(x_t(x) - \sqrt{\bar{\alpha}_t} \varphi(0)\right) W(\varphi | x_t, x) = 0.
    \]
    
    On remarque que :    
    \[
    \lim\limits_{t \to 0} W(\varphi | x_t, x) =
    \begin{cases} 
    1, & \text{si } \varphi = \tilde{\varphi} \coloneqq \arg\min\limits_{\psi \in P_{\Omega}^{x}} \|x_{\Omega_{x}|_{t=0}} - \psi \|^2, \\
    0, & \text{sinon}.
    \end{cases}
    \]
    
    Donc, la limite de la somme se réduit à :    
    \[
    (x_0(x) - \tilde{\varphi}(0)).
    \]
    
    La somme s’annule à \( t \to 0 \) implique que :    
    \[
    x_0(x) = \tilde{\varphi}(0).
    \]
\end{proof}




\appendix

\section{Proof of \cref{prop:fokker} \label{sec:proof_focker}}

\begin{proof}
Commençons par un rappel sur la formule d'Itô . 
\begin{theorem}[Formule d'Itô \cite{Oksendal2003}]
Soit \( X(t) \) un processus d’Itô de dimension \( n \) vérifiant  
\[
dX(t) = u(t, X(t)) dt + v(t, X(t)) dB(t),
\]
où :
\begin{itemize}
    \item \( X(t) \in \mathbb{R}^n \) est le processus d’état,
    \item \( u(t, X(t)) \in \mathbb{R}^n \) est le terme de dérive,
    \item \( v(t, X(t)) \in \mathbb{R}^{n \times m} \) est la matrice de diffusion,
    \item \( B(t) \in \mathbb{R}^m \) est un mouvement brownien standard de dimension \( m \).
\end{itemize}

Soit \( g: [0, \infty) \times \mathbb{R}^n \to \mathbb{R}^p \) une fonction deux fois continûment différentiable.  
On définit le processus transformé :
\[
Y(t) = g(t, X(t)),
\]
où \( Y(t) \in \mathbb{R}^p \).  
Alors, \( Y(t) \) satisfait l’équation d’Itô :
\[
dY_k = \frac{\partial g_k}{\partial t} dt 
+ \sum\limits_{i=1}^{n} \frac{\partial g_k}{\partial x_i} u_i dt 
+ \sum\limits_{i=1}^{n} \sum\limits_{j=1}^{m} \frac{\partial g_k}{\partial x_i} v_{ij} dB_j
+ \frac{1}{2} \sum\limits_{i,j=1}^{n} \sum\limits_{r=1}^{m} v_{ir} v_{jr} \frac{\partial^2 g_k}{\partial x_i \partial x_j} dt.
\]
\end{theorem}
\vspace{3em}



Soit une fonction test $F: \mathbb{R}^N \rightarrow \mathbb{R}$, de classe $C^{\infty}$, à support compact. En utilisant la formule d'Ito, on obtient
%     \[dF(x_t) = \sum\limits_{i=1}^N \frac{\partial F(x_t)}{\partial x_i} dx_t^i + \frac{1}{2} \sum\limits_{i=1}^N \sum\limits_{j=1}^N \frac{\partial^2 F(x_t)}{\partial x_i^2}d\langle x^i,x^j\rangle_t\]
% En utilisant l'équation \eqref{eq:SDE} et en notant que 
%  on obtient:
\begin{align*}
    dF(x_t) &= \sum\limits_{i=1}^N \frac{\partial F(x_t)}{\partial x_i} (f^i_t(x_t)dt + \mathcal{G}_tdW^i_t) + \frac{1}{2} \sum\limits_{i=1}^N  \frac{\partial^2 F(x_t)}{\partial x_i^2} \mathcal{G}_t^2 dt \\
    &= \left(\sum\limits_{i=1}^N \frac{\partial F(x_t)}{\partial x_i} f^i_t(x_t) + \frac{1}{2} \sum\limits_{i=1}^N  \frac{\partial^2 F(x_t)}{\partial x_i^2} \mathcal{G}_t^2\right)dt + \sum\limits_{i=1}^N \frac{\partial F(x_t)}{\partial x_i}  \mathcal{G}_t^2dW^i_t
\end{align*}

Le terme $dW_t^i$ disparaît en prenant l'espérance  (car $\mathbb{E}[dW_t^i] = 0$ ), donc :
\[\E{dF(x_t)} = \E{\left(\sum\limits_{i=1}^N \frac{\partial F(x_t)}{\partial x_i} f^i_t(x_t) + \frac{1}{2} \sum\limits_{i=1}^N \frac{\partial^2 F(x_t)}{\partial x_i^2} \mathcal{G}_t^2\right)dt}\]

Ou encore, en utilisant la linéarité de l'opérateur espérance, on peut sortir la dérivée, l'équation ci-dessus s'écrit :
\[ d\E{F(x_t)} = \E{\sum\limits_{i=1}^N \frac{\partial F(x_t)}{\partial x_i} f^i_t(x_t) + \frac{1}{2} \sum\limits_{i=1}^N \frac{\partial^2 F(x_t)}{\partial x_i^2} \mathcal{G}_t^2}dt\]

Ainsi,
\begin{align}
    \frac{d\E{F(x_t)}}{dt} &= \E{\sum\limits_{i=1}^N \frac{\partial F(x_t)}{\partial x_i} f^i_t(x_t) + \frac{1}{2} \sum\limits_{i=1}^N  \frac{\partial^2 F(x_t)}{\partial x_i^2}\mathcal{G}_t^2} \nonumber \\
    &=\E{\sum\limits_{i=1}^N \frac{\partial F(x_t)}{\partial x_i} f^i_t(x_t)} + \E{\frac{1}{2} \sum\limits_{i=1}^N  \frac{\partial^2 F(x_t)}{\partial x_i^2} \mathcal{G}_t^2} \nonumber\\
    &=\E{ \nabla\ F(x_t) \cdot f_t(x_t)} + \frac{1}{2}\E{\nabla^2 F(x_t) \mathcal{G}_t^2} \label{eq:EsperanceEgality}
\end{align}

$F$ est continue, donc mesurable, supposons que $x \rightarrow \frac{\partial\pi_t(x)}{\partial t}$ existe, $x \rightarrow F(x)\frac{\partial\pi_t(x)}{\partial t}$ est intégrable en $\R^N$ car F est à support compact. D'après la règle de Leibniz, le premier terme à droite s'écrit :
\[\frac{d\mathbb{E}[F(x_t)]}{dt} = \frac{d (\int_{\mathbb{R^N}} F(x) \pi_t(x) dx)} {dt} = \int_{\mathbb{R^N}}F(x)\frac{\partial\pi_t(x)}{\partial t} dx\]
\vspace{2em}

Nous allons rappeler la première identité de Green (Green's first identity) qui est fort utile dans la suite.

\begin{theorem}[Première identité de Green]
Soit $\Omega \subset \R^n$, on se donne une fonction $u : \Omega \rightarrow \R$, $u \in C^2$ et une autre fonction $v : \Omega \rightarrow \R$, $v \in C^1$, alors :
\begin{equation}\label{eq:GreenId}
\int_\Omega (v(x)\nabla^2 u(x) + \nabla u(x) \cdot \nabla v(x)) dx = \int_{\partial\Omega} v \nabla u \cdot n dS
\end{equation}
Avec n le vecteur normal unitaire à la frontière $\partial\Omega$ et $dS$ la mesure de frontière sur $\partial\Omega$.
\end{theorem}
\vspace{3em}

Supposons qu'il existe une fonction $v_t$ telle que $\nabla_{x} v_t = f_t \pi_t$.

On utilise la première identité de Green sur le premier terme à gauche en remarquant que l'intégrale sur le bord s'annule car F est à support compact, on obtient :
\begin{align*}
    \E{\nabla\ F(x_t)\cdot f_t(x_t)}  &=\int_{\mathbb{R^N}} \nabla\ F(x)\cdot \big(f_t(x) \pi_t(x) \bigr)dx \\
    &= - \int_{\mathbb{R}^N}F(x) (\nabla \cdot f_t(x)\pi_t(x))dx
\end{align*}

De la même manière, en appliquant l'intégration par parties deux fois sur le deuxième terme à droite, on obtient :
\begin{align*}
    \E{\nabla^2 F(x_t) \mathcal{G}_t^2} &= \int_{\mathbb{R}^N } \nabla^2 F(x) \mathcal{G}_t^2 \pi_t(x)dx\\
    &= -\int_{\R^N}\nabla F(x) \cdot\nabla(\mathcal{G}_t^2\pi_t(x)) dx\\
    &= \int_{\mathbb{R}^N } F(x) \nabla^2(\mathcal{G}_t^2 \pi_t(x)) dx
\end{align*}

Injectons les résultats précédents dans l'équation \eqref{eq:EsperanceEgality} :
\[\int_{\mathbb{R^N}}F(x)\frac{\partial\pi_t(x)}{\partial t} dx = - \int_{\mathbb{R}^N}F(x) (\nabla \cdot f_t(x)\pi_t(x))dx + \frac{1}{2}\int_{\mathbb{R}^N } F(x) \nabla^2(\mathcal{G}_t^2 \pi_t(x)) dx\]

Cette relation est vraie pour toute fonction de test $F \in C_c^{\infty}$ (i.e ensemble des fonctions infiniment dérivables à support compact), la théorie des distributions nous dit : 
\[\frac{\partial\pi_t(x)}{\partial t} = -\nabla \cdot (f_t(x)\pi_t(x)) + \frac{1}{2}\nabla^2(\mathcal{G}_t^2 \pi_t(x)) \quad \] 
\end{proof}

\section{Proof of \cref{prop:solution_processus_direct}} \label{sec:proof_solution_processus_direct}

\begin{proof}
    D'après l'équation \eqref{eq:OU}:
    \[dx_t^i = -\gamma_t x_t^i dt + \sqrt{2\gamma_t}dW_t^i, \quad \forall i=1,2,\dots,N\] 
    
    Posons $\mu_t = \exp{\left(\int_0^t \gamma_s ds\right)}, \quad \forall t \geq 0$.
    
    Soit $Y_t$ un processus stochastique défini par $Y_t \coloneqq f(t,x_t^i)= \mu_t x_t^i$.
    
    En utilisant la formule d'Ito, on obtient:
    \[dY_t = \frac{\partial f(t,x_t^i)}{\partial t} dt + \frac{\partial f(t,x_t^i)}{\partial x} dx_t + \frac{1}{2}\frac{\partial^2f(t,x_t^i)}{\partial x^2}d\langle x,x \rangle_t\]

    En particulier:
    \begin{itemize}
        \item $\frac{\partial f(t,x_t^i)}{\partial t} = \gamma_t \mu_t x_t^i$
        \item $\frac{\partial f(t,x_t^i)}{\partial x} = \mu_t$
        \item $\frac{\partial^2f(t,x_t^i)}{\partial x^2}=0$
    \end{itemize}

    Ainsi,
    \begin{align*}
        dY_t &= \gamma_t \mu_t x_t^i dt + \mu_t dx_t^i\\
        &= \gamma_t \mu_t x_t^i dt + \mu_t (-\gamma_t x_t^i dt + \sqrt{2\gamma_t}dW_t^i)\\
        &= \mu_t\sqrt{2\gamma_t}dW_t^i
    \end{align*}
    
    On passe à l'intégrale pour 2 côtés :
    \[Y_t = Y_0 + \int_0^t \mu_s\sqrt{2\gamma_s}dW_s^i\]

    Remplaçons $Y_t$ par $\mu_t x_t^i$, puis divisons les 2 côtés par $\mu_t$ pour faire apparaitre $x_t^i$, on obtient:
    \begin{align*}
        x_t^i &= \frac{1}{\mu_t}x_0^i + \frac{1}{\mu_t} \int_0^t \mu_s\sqrt{2\gamma_s}dW_s^i \\
        &= \sqrt{\bar{\alpha_t}}x_0^i + \frac{1}{\mu_t} \int_0^t \mu_s\sqrt{2\gamma_s}dW_s^i
    \end{align*}

    Notons $Z_t = \int_0^t\mu_s\sqrt{2\gamma_s}dW_s^i$. On remarque que $Z_t$ est une intégrale d'Itô par rapport à un mouvement brownien. On peut réécrire cette intégrale sous forme :
    \[Z_t=\int_0^t\mu_s\sqrt{2\gamma_s}dW_s^i = \lim\limits_{h\rightarrow 0} \sum\limits_{j=1}^{n} \mu_{t_j}\sqrt{2\gamma_{t_j}}(W^i_{t_j}-W^i_{t_{j-1}})\]
    Avec $t_0=0 < t_1<\dots<t_{n-1} < t_{n} = t$ une sous-division de l'intervalle $[0,t]$ et $h \coloneqq max_{j = 1,2,\dots,n} |t_j - t_{j-1}|$

    Comme les incréments d'un mouvement brownien sont les variables gaussiennes centrées et indépendantes l'une de l'autre, $Z_t$ est une variable gaussienne centrée dont la variance vaut $\int_0^t(\mu_s\sqrt{2\gamma_s})^2 ds $. On va calculer cette variance :
    \begin{align*}
        \int_0^t(\mu_s\sqrt{2\gamma_s})^2ds &= \int_0^t \exp\left(2 \int_0^s \gamma_xdx\right) 2 \gamma_s ds \\ 
        &=\int_0^t \exp\left(2 \int_0^s \gamma_xdx\right) d\left(2 \int_0^s \gamma_xdx\right)\\
        &= \exp\left(2 \int_0^t \gamma_xdx\right) -1
    \end{align*}
    
    Alors, $Z_t$ représente une variable gaussienne telle que  $Z_t \sim \mathcal{N}\left(0, \exp\left(2 \int_0^t \gamma_xdx\right) -1\right) $.
    
    Par conséquent, 
    \[\frac{1}{\mu_t}Z_t \sim \mathcal{N}\left(0, 1-\exp\left(-2 \int_0^t \gamma_xdx\right)\right)\]
    
    D'où :
    \[\frac{1}{\mu_t}Z_t \sim \mathcal{N}\left(0, 1-\bar{\alpha_t}\right)\]

    On injecte ce résultat dans l'équation de $x_t^i$, on obtient :
    \[x_t^i = \sqrt{\bar{\alpha_t}}x_0^i + \sqrt{1-\bar \alpha_t}\,\eta^i\]
    avec $\eta^i \sim \sqrt{1-\bar{\alpha_t}}\,\mathcal{N}\left(0, 1\right)$

    On obtient $x_t$ en rassemblant les expressions de ses composantes. 
    \[x_t = \sqrt{\bar{\alpha_t}}x_0 + \sqrt{1-\bar{\alpha_t}}\eta_t \quad \] 
    Avec $\eta$ est définie telle que chaque composante de $\eta$ a une distribution normale centrée réduite. L'espérance de $\eta$ est donc $0_{\R^N}$. Il nous reste à chercher sa matrice de covariance.

    On remarque  que chaque composante $\eta^i$ de $\eta$ ne dépend que de $W_t^i$, de plus, $W_t^i$ est à son tour indépendante de l'autre composante de $W_t$. On peut donc conclure que les composantes de $\eta$ sont indépendantes l'une de l'autre. $\eta$ est donc un vecteur gaussien isotrope car ses composantes sont des variables indépendamment et identiquement distribuées i.e
    \[\eta \sim \mathcal{N}(0_{\R^N},I_N)\]
\end{proof}


\section{Proof of \cref{prop:quelques_resultats_sur_la_densite}} \label{sec:proof_quelques_resultates_sur_la_densite}
\begin{proof}\
    (i) La fonction de répartition de $U$ s'écrit :
    \begin{align*}
        F_U(x) &= \mathbb{P}(U_1 \leq x_1;U_2 \leq x_2;\dots;U_n \leq x_n) \\
        &=\mathbb{P}\left(X_1 \leq \frac{x_1}{ \alpha};X_2 \leq \frac{x_2}{\alpha};\dots;X_n \leq \frac{x_n}{ \alpha}\right) \\
        &= F_X\left(\frac{x}{\alpha}\right)
    \end{align*}

    Pour avoir la fonction de répartition $f_U$ de $U$, on passe l'équation ci-dessus à la dérivée :
    \begin{align*}
        f_U(x) &= \frac{\partial^nF_U}{\partial z_1 \partial z_2 \dots\partial z_1}(x) \\
        &= \frac{\partial^nF_X}{\partial z_1 \partial z_2 \dots\partial z_1}\left(\frac{x}{ \alpha}\right) \left(\frac{1}{ \alpha}\right)^n \\
        &=\left(\frac{1}{ \alpha}\right)^n f_X\left(\frac{x}{\alpha}\right)
    \end{align*}

    (ii)La fonction de répartition de \( \mathbf{Z} \) est donnée par :
    \[
    F_Z(\mathbf{z}) = \mathbb{P}(\mathbf{Z} \leq \mathbf{z}) = \mathbb{P}(\mathbf{X} + \mathbf{Y} \leq \mathbf{z}),
    \]
    où l'inégalité est comprise composante par composante, c'est-à-dire \( Z_i \leq z_i \) pour tout \( i = 1,2, \dots, n \).
    
    Comme \( \mathbf{X} \) et \( \mathbf{Y} \) sont indépendants, nous pouvons exprimer cette probabilité sous forme intégrale :
    \[
    F_Z(\mathbf{z}) = \int_{\mathbb{R}^n} \mathbb{P}(\mathbf{X} \leq \mathbf{z} - \mathbf{y}) f_Y(\mathbf{y}) \, d\mathbf{y}.
    \]
    
    Or, par définition de la fonction de répartition de \( \mathbf{X} \), on a :
    \[
    F_X(\mathbf{z} - \mathbf{y}) = \mathbb{P}(\mathbf{X} \leq \mathbf{z} - \mathbf{y}),
    \]
    
    d'où :
    \[
    F_Z(\mathbf{z}) = \int_{\mathbb{R}^n} F_X(\mathbf{z} - \mathbf{y}) f_Y(\mathbf{y}) \, d\mathbf{y}.
    \]
    
    Pour obtenir la densité \( f_Z(\mathbf{z}) \), nous dérivons des deux côtés par rapport à \( \mathbf{z} \) :
    \[
    f_Z(\mathbf{z}) = \frac{\partial^n}{\partial z_1 \dots \partial z_n} \int_{\mathbb{R}^n} F_X(\mathbf{z} - \mathbf{y}) f_Y(\mathbf{y}) \, d\mathbf{y}.
    \]
    
    Comme $\frac{\partial^n}{\partial z_1 \dots \partial z_n} F_X \equiv f_X$ existe, en appliquant la règle de Leibniz (différentiation sous le signe intégral), on obtient :
    \[
    f_Z(\mathbf{z}) = \int_{\mathbb{R}^n} \frac{\partial^n}{\partial z_1 \dots \partial z_n} F_X(\mathbf{z} - \mathbf{y}) f_Y(\mathbf{y}) \, d\mathbf{y}.
    \]
    
    Or, la dérivée de la fonction de répartition \( F_X \) est la densité \( f_X \), donc :
    \[
    f_X(\mathbf{z} - \mathbf{y}) = \frac{\partial^n}{\partial z_1 \dots \partial z_n} F_X(\mathbf{z} - \mathbf{y}).
    \]
    
    Ainsi, on obtient :
    \[
    f_Z(\mathbf{z}) = \int_{\mathbb{R}^n} f_X(\mathbf{z} - \mathbf{y}) f_Y(\mathbf{y}) \, d\mathbf{y}.
    \]
\end{proof}

\section{Proof of \cref{prop:distribution_a_etap_t}} \label{sec:proof_distribution_a_etap_t}
\begin{proof}
    On commence par rappeler le processus direct:
    \begin{align*}
    x_t &= \sqrt{\bar \alpha_t}x_0 + \sqrt{1-\bar \alpha_t} \eta \quad \text{avec } \eta \sim \mathcal{N}(0_{\R^N},I_N)\\
    \end{align*}

    Posons $X = \sqrt{\bar \alpha_t}x_0$ et $Y = \sqrt{1-\bar \alpha_t} \eta$. Comme $x_0$ et $\eta$ sont indépendantes, $X$ et $Y$ le sont. En utilisant le résultat (i) de \cref{prop:quelques_resultats_sur_la_densite}, on obtient la fonction de densité de $X$ :
    \[f_X(x) = \frac{1}{\sqrt{\bar{\alpha_t}}^N} \pi_0\left(\frac{x}{\sqrt{\bar{\alpha_t}}}\right) \quad\forall x \in \R^N\]

    La fonction de densité de $Y$ est $g$ qui est celle d'une loi normale $\mathcal{N}\left(0_{\R^N}, (1-\bar \alpha _t\,)I_N\right)$.

    D'après le résultat (ii) de la  \cref{prop:quelques_resultats_sur_la_densite}, la fonction de densité $\pi_t$ de $x_t$ est donnée par :
    \begin{align*}
        \pi_t(x) &= (f_X*g)(x) \quad \forall x\in \R^N \\
        &= \int_{\R_N} \frac{1}{\sqrt{\bar{\alpha_t}}^N} \pi_0\left(\frac{z}{\sqrt{\bar{\alpha_t}}}\right) \mathcal{G}_t(x -z) dz\\
    \end{align*}

    On remarque $\frac{1}{\sqrt{\bar{\alpha_t}}^n} dz = d\left(\frac{z}{\sqrt{\bar{\alpha_t}}}\right)$. En faisant une changement de variable $\omega = \frac{\omega}{\sqrt{\bar{\alpha_t}}}$, la distribution de $x_t$ s'écrit :
    \begin{align*}
        \pi_t(x) &= (f_X*g)(x) \quad \forall x\in \R^N \\
        &= \int_{\R_N}  \pi_0(\omega) \mathcal{G}_t(x -\sqrt{\bar{\alpha_t}}\,\omega) d\omega\\
    \end{align*}
\end{proof}

% \section{Proof of \cref{prop:score_a_esperance_conditionnelle}}\label{sec:proof_score_a_esperance_conditionnelle}
% \begin{proof}
%     L'espérance conditionnelle mentionnée ci-dessus s'écrit :
%     \begin{align}
%         \E{\eta|x_t = x} &= \int_{\R^N} z f_{\eta|x_t=x}(z) dz \nonumber\\
%         &= \int_{R^N} z \frac{f_{\eta,x_t}(z,x)}{\pi_t(x)}dz \label{eq:first_developpement} 
%     \end{align}

%     La densité de probabilité jointe de $\eta$ et $x_t$ est donnée par :
%     \begin{align*}
%         f_{\eta,x_t}(z,x) &= f_{\eta,\sqrt{\bar \alpha_t}x_0 + \sqrt{1-\bar\alpha_t}\eta }(z,x) \\
%         &= f_{\eta,x_0}\left(z,\frac{x-\sqrt{1-\bar \alpha_t} z}{\sqrt{\bar \alpha_t}}\right)
%     \end{align*}

%     Et comme $\eta$ et $x_0$ sont indépendantes, la densité jointe est égale au produit de deux densités, on obtient :
%     \begin{equation*}
%         f_{\eta,x_t} = f_{\mathcal{N}(0,I_N)}(z) \,\pi_0\left(\frac{x-\sqrt{1-\bar \alpha_t} z}{\sqrt{\bar \alpha_t}}\right)
%     \end{equation*}

%     Injectons ce résultat dans l'\cref{eq:first_developpement} :
%     \begin{equation*}
%         \E{\eta|x_t = x}=\int_{\R^N} z \,\frac{f_{\mathcal{N}(0,I_N)}(z) \,\pi_0\left(\frac{x-\sqrt{1-\bar \alpha_t} z}{\sqrt{\bar \alpha_t}}\right)}{\pi_t(x)}\, dz
%     \end{equation*}

%     On fait un changement de variable, posons $z = \frac{x-\sqrt{1-\bar \alpha_t} z}{\sqrt{\bar \alpha_t}}$. De cette manière, on obtient :
%     \begin{equation*}
%         \begin{cases}    
%             z = \frac{x-\sqrt{\bar \alpha_t}z}{\sqrt{1-\bar\alpha_t}} \\
%             dz = \left| \frac{\sqrt{\bar \alpha_t}}{\sqrt{1-\bar\alpha_t}}\right|^N dz = \left( \frac{\bar \alpha_t}{1-\bar\alpha_t}\right)^\frac{N}{2}dz
%         \end{cases}
%     \end{equation*}

%     Ainsi,
%     \begin{equation*}
%         \E{\eta|x_t = x} = \int_{\R^N} \frac{x-\sqrt{\bar \alpha_t}\,z}{\sqrt{1-\bar\alpha_t}}\, \frac{f_{\mathcal{N}(0,I_N)}\left(\frac{x-\sqrt{\bar \alpha_t}\,z}{\sqrt{1-\bar\alpha_t}}\right)\, \pi_0(z)}{\pi_t(x)}\,\left( \frac{\bar \alpha_t}{1-\bar\alpha_t}\right)^\frac{N}{2} dz
%     \end{equation*}

%     Ou encore, par le point (i) de la \cref{prop:quelques_resultats_sur_la_densite} :
%     \begin{align*}
%         \left( \frac{1}{1-\bar\alpha_t}\right)^\frac{N}{2}f_{\mathcal{N}(0,I_N)}\left(\frac{x-\sqrt{\bar \alpha_t}z}{\sqrt{1-\bar\alpha_t}}\right) &=  f_{\mathcal{N}(0,(1-\bar \alpha_t)I_N)}(x-\sqrt{\bar \alpha_t}z) \\
%         &= g(x-\sqrt{\bar \alpha_t}\,z)
%     \end{align*}

%     D'où :
%     \begin{align*}
%         \E{\eta|x_t = x} &= \frac{\bar \alpha_t^{\frac{N}{2}}}{\sqrt{1-\bar\alpha_t}} \int_{\R^N} \frac{\pi_0(z)\, g(x - \sqrt{\bar \alpha_t} \,z)(x - \sqrt{\bar \alpha_t} \,z)}{\pi_t(x)} dz \\
%         % &= - \bar \alpha_t^{\frac{N}{2}} \sqrt{1-\bar\alpha_t}\, s_t(x)
%     \end{align*}
    
% \end{proof}

\section{Proof of \cref{prop:Groupe_unitaire}}\label{sec:proof_groupe_unitaire}
\begin{proof}
    Démontrons les propriétés une par une.

    \begin{enumerate}[label=(\roman*)]
        \item Soit $x \in X$, on a :
        \begin{equation*}
            \langle Ux, U y \rangle = \langle x, U^\dagger U y \rangle
        \end{equation*}
        
        Ou encore, par définition $\langle Ux, U y \rangle= \langle x, y \rangle $.

        Ainsi,
        \begin{equation*}
            \langle x, y \rangle = \langle x, U^\dagger U y \rangle
        \end{equation*}

        Ce qui entraîne $U^\dagger U = e$. Par unicité de la transformation inverse de $U$, on déduit :
        \[
        U^{-1} = U^\dagger.
        \]
        \item Soit \( x \in X \). Comme \( U \) est unitaire, on a :
        \[
        \|Ux\|^2 = \langle Ux, Ux \rangle = \langle x, U^\dagger U x \rangle = \langle x, x \rangle = \|x\|^2.
        \]
        En prenant la racine carrée, on obtient :
        \[
        \|Ux\| = \|x\|.
        \]

        \item Soit \( f \in C^1(X, \R) \) et \( x \in X \). On veut montrer que :
        \[
        \nabla_x f(U^{-1}x) = U \, \nabla f(U^{-1}x).
        \]
        Par définition du gradient, pour tout vecteur \( h \in X \), on a :
        \[
        \langle \nabla_x f(U^{-1}x), h \rangle = D_x f(U^{-1}x) \cdot h,
        \]
        où \( D_x f(U^{-1}x) \) est la dérivée directionnelle de \( f \) en \( U^{-1}x \) dans la direction \( h \). En utilisant la règle de dérivation en chaîne, on obtient :
        \[
        D_x f(U^{-1}x) \cdot h = D f(U^{-1}x) \cdot (U^{-1} h) = \langle \nabla f(U^{-1}x), U^{-1} h \rangle.
        \]
        Comme \( U \) est unitaire, \( U^{-1} = U^\dagger \), donc :
        \[
        \langle \nabla f(U^{-1}x), U^{-1} h \rangle = \langle U \nabla f(U^{-1}x), h \rangle.
        \]
        Ainsi, on a :
        \[
        \langle \nabla_x f(U^{-1}x), h \rangle = \langle U \nabla f(U^{-1}x), h \rangle.
        \]
        Comme cette égalité est vraie pour tout \( h \in X \), on en déduit que :
        \[
        \nabla_x f(U^{-1}x) = U \, \nabla f(U^{-1}x).
        \]
    \end{enumerate}
\end{proof}

% \section{Proof of \cref{theo:equivariant_modele}}\label{sec:proof_equivariant_modele}
% \begin{proof}
% Soit $M_t$ G-équivariant modèle qui approxime la fonction du score $s_t$. $M_t$ est donc optimisé sous la fonction de coût suivante:
% \[
% \mathcal{L}_t(M_t) = \mathbb{E}_{x \sim \pi_t} \left[ \| M_t(x) - s_t(x) \|^2 \right]
% \]

% On fixe un point $\psi \in \mathbb{R}^N$. Par définition, l'orbite de $\psi$ sous le groupe $G$ est $G(\psi) = \{ x |\, \exists U \in G, U \psi = x \}$. Comme $M_t$ est G-équivariant modele, pour tout $x \in G(\psi)$, $\exists U \in G$ tel que $M_t(x) = U M_t(\psi)$.

% Le problème d'optimiser $M_t(x)$ pour tout $x \in G(\psi)$ revient à chercher $M_t$ minimiser la fonction de coût suivante:
% \begin{align*}
% \widetilde{\mathcal{L}}_t(M_t) &= \mathbb{E}_{x_t \sim \pi_t | x_t \in G(\psi)} \left[ \| M_t(x_t) - \nabla \log \pi_t(x_t) \|^2 \right]\\ 
% &= \mathbb{E}_{x_t \sim \pi_t} \left[ \| M_t(x_t) - \nabla \log \pi_t(x_t) \|^2 \,|\, x_t \in G(\psi) \right] \\
% &= \frac{1}{\mathbb{P}_{x_t \sim \pi_t} (x_t \in G(\psi))}\,\mathbb{E}_{x_t \sim \pi_t} \left[ \| M_t(x_t) - \nabla \log \pi_t(x_t) \|^2\, \mathbb{1}_{x_t \in G(\psi)} \right] \\
% &= \frac{1}{\mathbb{P}_{x_t \sim \pi_t} (x_t \in G(\psi))} \int_{G(\psi)} \pi_t(z) \| M_t(z) - \nabla \log \pi_t(z) \|^2 \, dz
% \end{align*}

% On va rappeler la propriété de stabilisateur orbital qui nous permet de convertir l'intégrale sur l'orbite à l'intégrale sur le groupe entier. Mathématiquement, la propriété s'annonce que  
% \begin{align*}
%     & \int_{G(\psi)} u(z) \, dz = \int\limits_{G/Stab(\psi)} u(U\psi) \, dU\\
%     & \text{avec } Stab(\psi) \coloneqq \{U\in G \, | U\psi = \psi\}
% \end{align*}


% Ou encore,
% \[
% \int_{G/Stab(\psi)} u(U\psi) \, dU = \int_{G/Stab(\psi)} u(U^{-1}\psi) \, d(U^{-1}) \quad \text{car } \forall U \in G, \exists ! U \in G.
% \]
% % \todo{Parler de la mesure de Haar}

% De plus, par l'invariance de la mesure de Haar, on a $dU = dU^{-1}$. Ainsi,
% \[
% \int_{G(\psi)} u(z) \, dz = \int_{G/Stab(\psi)} u(U^{-1}\psi) \, dU
% \]

% Injectons ce résultat dans l'expression de $\widetilde{\mathcal{L}}_t$, on obtient :
% \[
% \widetilde{\mathcal{L}}(M_t) = \frac{1}{\mathbb{P}_{x_t \sim \pi_t} (x_t \in G(\psi))} \int_{G/Stab(\psi)} \pi_t(U^{-1}\psi) \| M_t(U^{-1}\psi) - \nabla \log \pi_t(U^{-1}\psi) \|^2 \, dU
% \]

% Tout élément $U$ du groupe $G$ préserve la norme, c'est-à-dire $\|Ux\| = \|x\|$, on en déduit:

% \[
% \widetilde{\mathcal{L}}_t(M_t) = \frac{1}{\mathbb{P}_{x_t \sim \pi_t} (x_t \in G(\psi))} \int_{G/Stab(\psi)} \pi_t(U^{-1}\psi) \|UM_t(U^{-1}\psi) - U\nabla \log \pi_t(U^{-1}\psi)\|^2 dU
% \]

% Comme $M_t$ est un $G$-équivariant modèle, on a:
% \[
% UM_t(U^{-1}\psi) = UU^{-1}M_t(\psi) = M_t(\psi).
% \]

% Ainsi,
% \[
% \widetilde{\mathcal{L}}_t(M_t) = \frac{1}{\mathbb{P}_{x_t \sim \pi_t} (x_t \in G(\psi))} \int_{G/Stab(\psi)} \pi_t(U^{-1}\psi) \|M_t(\psi) - U\nabla \log \pi_t(U^{-1}\psi)\|^2 dU.
% \]

% On ajoute une petite perturbation $\varepsilon h$ à $\widetilde{\mathcal{L}}_t$ avec $\varepsilon \in \R$ et $h$ une fonction arbitraire, ce qui nous donne :
% \begin{align*}
%     \widetilde{\mathcal{L}}_t(M_t + \varepsilon h) &= \frac{1}{\mathbb{P}_{x_t \sim \pi_t} (x_t \in G(\psi))} \int_{G/Stab(\psi)} \pi_t(U^{-1}\psi) \|M_t(\psi) + \varepsilon h(\psi) - U\nabla \log \pi_t(U^{-1}\psi)\|^2 dU\\
%     &= \widetilde{\mathcal{L}}_t(M_t) + 2\varepsilon \frac{1}{\mathbb{P}_{x_t \sim \pi_t} (x_t \in G(\psi))} \int_{G/Stab(\psi)} \pi_t(U^{-1}\psi) \langle h(\psi), M_t(\psi) - U\nabla \log \pi_t(U^{-1}\psi) \rangle dU + O(\varepsilon^2)
% \end{align*}

% La variation de premier ordre de $\widetilde{\mathcal{L}}_t$ correspond à:
% \begin{align*}
%     \delta \widetilde{\mathcal{L}}_t(M_t) &= \lim\limits_{\varepsilon \to 0} \frac{\widetilde{\mathcal{L}}_t(M_t + \varepsilon h) - \widetilde{\mathcal{L}}_t(M_t)}{\varepsilon}\\
%     &= \frac{2}{\mathbb{P}_{x_t \sim \pi_t} (x_t \in G(\psi))} \int_{G/Stab(\psi)} \pi_t(U^{-1}\psi) \langle h(\psi), M_t(\psi) - U\nabla \log \pi_t(U^{-1}\psi) \rangle dU
% \end{align*}



% $M_t$ est l'optimal fonctionnel de $\widetilde{\mathcal{L}}_t$ convexe ssi $\delta \widetilde{\mathcal{L}}_t(M_t) = 0$ pour toute perturbation fonctionnelle $h$, ce qui entraîne que:
% \[
% \int_{G/Stab(\psi)} \pi_t(U^{-1}\psi) (M_t(\psi) - U\nabla \log \pi_t(U^{-1}\psi)) dU = 0_{\mathbb{R}^N}
% \]

% Ainsi:
% \begin{align*}
%     M_t(\psi) &= \frac{\int_{G/Stab(\psi)} U\nabla \log \pi_t(U^{-1}\psi) \pi_t(U^{-1}\psi) dU}{\int_{G/Stab(\psi)} \pi_t(U^{-1}\psi) dU} \\
%     &= \frac{1}{\int_{G/Stab(\psi)} \pi_t(U^{-1}\psi) dU} \int_{G/Stab(\psi)} U \nabla_\psi \pi_t(U^{-1}\psi) dU
% \end{align*}


% En utilisant les points (i) et (iii) de la \cref{prop:Groupe_unitaire}, on déduit :
% \[
% M_t(\psi) = \frac{\int_{G/Stab(\psi)} \nabla_\psi \pi_t(U^{-1}\psi) dU}{\int_{G/Stab(\psi)} \pi_t(U^{-1}\psi) dU} = \nabla_\psi \log \int_{G/Stab(\psi)} \pi_t(U^{-1}\psi) dU
% \]

% Notons
% \[
% \int_{G/Stab(\psi)} \pi_t(U^{-1}\psi) dU = \frac{1}{|\mathcal{D}|} \sum\limits_{x_0  \in \mathcal{D}} \int_{G/Stab(\psi)} \mathcal{N}(U^{-1}\psi | \sqrt{\bar \alpha_t} \varphi, (1-\bar \alpha_t) I) dU
% \]

% Comme $U$ est unitaire, il en résulte que :
% \begin{align*}
%     \mathcal{N}(U^{-1}\psi | \sqrt{\bar \alpha_t} \varphi, (1-\bar \alpha_t) I) &\propto \exp \left( -\frac{\|U^{-1}\psi - \sqrt{\bar \alpha_t} \varphi\|^2}{2(1-\bar \alpha_t)} \right) \\
%     & \propto \exp \left( -\frac{\|\psi - \sqrt{\bar \alpha_t} U \varphi\|^2}{2(1-\bar \alpha_t)} \right)
% \end{align*}




% Alors
% \[
% \int_{G/Stab(\psi)} \pi_t(U^{-1}\psi) dU = \frac{1}{|\mathcal{D}|} \sum\limits_{x_0  \in \mathcal{D}} \int_{G/Stab(\psi)} \mathcal{N}(\psi | \sqrt{\bar \alpha_t} U \varphi, (1-\bar \alpha_t) I) dU
% \]

% Par la propriété du stabilisateur orbital :
% \[
% \int_{G/Stab(\psi)} \pi_t(U^{-1}\psi) dU = \frac{1}{|\mathcal{D}|} \sum\limits_{x_0  \in \mathcal{D}} \int_{G(\varphi)} \mathcal{N}(\psi | \sqrt{\bar \alpha_t} \varphi', (1-\bar \alpha_t) I) d\varphi'
% \]

% D'où
% \begin{align*}
%     M_t(\psi) &= \nabla_\psi \log \int_{G/Stab(\psi)} \pi_t(U^{-1}\psi) d\psi\\
%     &= -\frac{1}{1-\bar \alpha_t} \frac{\sum\limits_{x_0  \in \mathcal{D}} \int_{G(\varphi)} (\psi - \sqrt{\bar \alpha_t} \varphi') \mathcal{N}(\psi | \sqrt{\bar \alpha_t} \varphi', (1-\bar \alpha_t) I) d\varphi'}{\sum\limits_{x_0  \in \mathcal{D}} \int_{G(\varphi)} \mathcal{N}(\psi | \sqrt{\bar \alpha_t} \varphi', (1-\bar \alpha_t) I) d\varphi'}
% \end{align*}
% \end{proof}

\section{Proof of \cref{theo:formule_de_Tweedie}}\label{sec:proof_formule_de_Tweedie}
\begin{proof}
Par le point (ii) de la proposition \cref{prop:quelques_resultats_sur_la_densite}, on obtient la distribution de $Y$:
\[ p_Y(y) = \int_{\mathbb{R}^n} f_X(x) \varphi_B(y - x) dx = (f_X * \varphi_B)(y) \]

En différenciant sous le signe intégral :
\begin{align*}
\nabla p_Y(y) &= \int_{\mathbb{R}^n} f_X(x) \nabla_y \varphi_B(y - x) dx \\
&= -\frac{1}{\sigma^2} \int_{\mathbb{R}^n} (y - x) f_X(x) \varphi_B(y - x) dx \\
&= -\frac{1}{\sigma^2} y \int_{\mathbb{R}^n} f_X(x) \varphi_B(y - x) dx + \frac{1}{\sigma^2} \int_{\mathbb{R}^n} x f_X(x) \varphi_B(y - x) dx \\
&= -\frac{1}{\sigma^2} y \cdot p_Y(y) + \frac{p_Y(y)}{\sigma^2} \int_{\mathbb{R}^n} \frac{x f_X(x) \varphi_B(y - x)}{p_Y(y)} dx
\end{align*}

On remarque que :
\begin{align*}
\frac{f_X(x) \varphi_B(y - x)}{p_Y(y)} &= \frac{f_{X,B}(x, y - x)}{p_Y(y)} = \frac{f_{X,Y}(x, y)}{p_Y(y)} \\
&= f_{X|Y=y}(x)
\end{align*}

Avec $f_{X,B}$ la densité jointe de $X$ et $B$, $f_{X|Y=y}$ la densité de $X$ sachant $Y = y$.

D'où :
\[ \nabla p_Y(y) = -\frac{1}{\sigma^2} y \cdot p_Y(y) + \frac{p_Y(y)}{\sigma^2} \mathbb{E}[X | Y = y] \]

En notant que $\nabla \log p_Y(y) = \frac{\nabla p_Y(y)}{p_Y(y)}$, on déduit :
\[ \mathbb{E}[X | Y = y] = y + \sigma^2 \nabla \log p_Y(y) \]
\end{proof}
\section{Proof of \cref{propr:normal_on_subspace}}\label{sec:proof_normal_on_subspace}
\begin{proof}
    Consider the isometry $\psi:\R^{N - K} \to  \mathcal{V}$, via orthogonal parameterization $V \in \R^{N \times (N - K)}$:
    \begin{equation*}
        \mathcal{V} = \psi(\R^{N-K}) = \left\{ v_0 + V u: u \in \R^{N - K} \right\}
    \end{equation*}
    Since $\psi$ is an affine isometry, we have $\det{J_{x} J_{x}^T} = 1$ and therefore we can relate the Hausdorff measure with the Lebesgue measure: 
    \begin{align*}
        \int_{\mathcal{V}} \Normal{w; \mu, \sigma^2 \Id_{N}} d \mathcal{H}^{N - K}(w)
        &=  \int_{\R^{N - K}} \Normal{\psi(u); \mu, \sigma^2 \Id_N} d u  \\
        &= \frac{1}{(2\pi\sigma^2)^{N/2}} \int_{\R^{N-K}} \exp \left( -\frac{\norm{v_0 + V u - \mu}^2}{2} \right) du \\
        &= \frac{1}{(2\pi \sigma^2)^{P/2}} \exp \left( -\frac{\norm{(\Id - V V^T)(\mu - v_0)}^2}{2\sigma^2} \right)\\
    \end{align*}
    The last equality is from the fact that: 
    \begin{align*}
        \norm{v_0+ V u - \mu}^2 &= \norm{v_0 - \mu}^2 + \norm{V u}^2 + 2 \inner{v_0 - \mu, V u} \\ 
        &= \norm{v_0 - \mu}^2 + \norm{u}^2 + 2 \inner{V^T(v_0 - \mu), u} \\
        &= \norm{v_0 - \mu}^2 + \norm{u + V^T(v_0 - \mu)}^2 - \norm{V^T(v_0 - \mu)}^2 \\
        &= \norm{(\Id_N - V V^T)(v_0 - \mu)}^2 + \norm{u + V^T(v_0 - \mu)}^2
    \end{align*}
\end{proof}

\bibliographystyle{plain}
\bibliography{bibliography} 
\end{document}